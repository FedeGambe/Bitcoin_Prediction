# -*- coding: utf-8 -*-
"""ProjectWork_Crif.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZLNtwSHd5MNxl6iJUWaGjxv-o84B_CB

# **Cripto Trend & Sentiment**

## **Project Work finale** – Corso Data Science and Generative AI (OVED627)  
**Durata:** 17/04/2025 – 18/04/2025

### **Obiettivo**

L'obiettivo dell'esercitazione è analizzare un dataset contenente i tick della criptovaluta **Bitcoin** per:

- Condurre un’analisi descrittiva dei dati
- Applicare modelli di Machine Learning per:
  - Prevedere il **Market Trend** (trend di mercato)
  - Stimare il **Fear & Greed Index** (indicatore del sentiment di mercato)

### **Parte 1** – Analisi Esplorativa

La prima fase prevede un’analisi descrittiva e di pulizia dei dati:

- Verifica della **presenza di valori nulli**
- Individuazione di **valori duplicati**
- Calcolo e rappresentazione di **correlazioni** tra variabili
- Visualizzazione delle distribuzioni e relazioni tra feature

Questa fase ha lo scopo di comprendere la struttura del dataset, la qualità del dato e le eventuali trasformazioni necessarie.



### **Parte 2 e 3** – Analisi Predittiva

Una volta preparati i dati, si procede alla fase di modellazione predittiva con tecniche di **Machine Learning**.

### Obiettivi:

- **Previsione del Market Trend**  
  Stimare l’andamento futuro del prezzo del Bitcoin tramite modelli di classificazione (es. salita/discesa del prezzo, o previsioni multiclasse).
  
- **Stima del Fear & Greed Index**  
  Utilizzare variabili quantitative e di sentiment per stimare il livello di paura o avidità del mercato, tramite modelli regressivi o classificatori.

### Approfondimento: Fear & Greed Index

Il *Fear & Greed Index* è un indicatore che misura il **sentiment degli investitori** nei mercati finanziari.  
Tenta di valutare se il comportamento degli operatori è dominato dalla **paura** (scarsa fiducia, vendite) o dall’**avidità** (ottimismo eccessivo, acquisti impulsivi).

###**Fasi di ricerca**

0. Importazione e pulizia del dato

1. Analisi descrittiva dell’andamento del Bitcoin

2. Analisi predittiva del market trend di Bitcoin tramite algoritmi ML/NN

3. Analisi predittiva del fear and greed index tramite algoritmi ML/NN
"""

#######################################################################################
# #                                                                                  ##
##   Per caricare i modelli LSTM e History scaricarli da GitHub e importarli         ##
##   link:                                                                           ##
##   https://github.com/FedeGambe/Bitcoin_Prediction/tree/main/Materiali/Modelli     ##
# #                                                                                  ##
#######################################################################################

"""## 0. Import"""

!pip install kaleido -q
import kaleido #required
kaleido.__version__ #0.2.1

import plotly
plotly.__version__ #5.5.0

import plotly.graph_objects as go
import plotly.express as px
import plotly.figure_factory as ff
from plotly.subplots import make_subplots

import pandas as pd
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
sns.set_theme()

#Tensorflow
import tensorflow
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.metrics import R2Score

import pickle

#SKLEARN
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor, QuantileRegressor, RANSACRegressor, TheilSenRegressor, LinearRegression, BayesianRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import KFold, cross_val_score, TimeSeriesSplit, GridSearchCV, train_test_split
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    average_precision_score,
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix
)

from lightgbm import LGBMClassifier

#Stats model: per VIF e Modelli Lineari
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import t

#############################
#   Import da Colab
#############################

#from google.colab import drive
#drive.mount('/content/drive')
#data_merged = pd.read_csv('/content/drive/MyDrive/Data Science/Machine e Deep Learning/OVED627/Dataset e consegna/merged_fix_to_hour.csv')

import zipfile
import os
import requests
from io import BytesIO

url = "https://github.com/FedeGambe/Bitcoin_Prediction/raw/main/Materiali/Datasets.zip"
response = requests.get(url) # Scarica il file ZIP

if response.status_code == 200: #check per vedere se è avvenuto con successo
    with zipfile.ZipFile(BytesIO(response.content)) as zip_ref: #temp folder per la decompressione
        zip_ref.extractall("cartella_decompressa") #estrazione file

        extracted_files = zip_ref.namelist() # Lista dei file estratti
        print(f"File estratti: {extracted_files}")

        csv_file_name = "merged_fix_to_hour.csv" #file da estrarre
        if csv_file_name in extracted_files:
            csv_file_path = os.path.join("cartella_decompressa", csv_file_name)
            data_merged = pd.read_csv(csv_file_path)
            print(f"\nPrimi 5 record del CSV '{csv_file_name}':")
            print(data_merged.head())
        else:
            print(f"Il file {csv_file_name} non è stato trovato nella cartella decompressa.")
else:
    print("Errore nel download del file ZIP.")

data_merged.info()

columns_BTC = [col for col in data_merged.columns if col.startswith('BTC') or col == 'Datetime']
df = data_merged[columns_BTC]
df.info()

"""## 1. Analisi Descrittiva

### 1.1. Analisi introduttiva del dataset
"""

columns_to_drop = [col for col in data_merged.columns
                   if 'high' in col.lower() or 'low' in col.lower() or 'open' in col.lower()]
data = data_merged.drop(columns=columns_to_drop)

data['Datetime'] = pd.to_datetime(data['Datetime'])

for col in ['funding_rate', 'fear_gread_index', 'google_trends_buy_crypto', 'google_trends_bitcoin']:
    data[col] = pd.to_numeric(data[col], errors='coerce')

print(f"Numero di righe: {data.shape[0]}")
print(f"Numero di colonne: {data.shape[1]}")

nan_values = data.isnull().sum()
if nan_values.sum() > 0:
    print("\nIl DataFrame contiene NaN values.")
    print("NaN values per variabili:\n", nan_values)
else:
    print("Il DataFrame non contiene NaN values.")

data.info()

print(f"""Il dataframe va dal giorno {data['Datetime'].min()} al giorno {data['Datetime'].max()}
per un totale di {(data['Datetime'].max() - data['Datetime'].min()).days} giorni
""")

"""### 1.2. Analisi dei gruppi di variabili

Il dataset è stato suddiviso in gruppi tematici che permettono di monitorare e analizzare vari aspetti dei mercati finanziari. I principali gruppi di dati comprendono criptovalute, materie prime, indici e indicatori macroeconomici.

**1. Criptovalute**:    Le criptovalute sono una classe di asset digitale sempre più rilevante nel panorama finanziario globale. Il gruppo cryptos_columns include dati relativi a diverse criptovalute importanti, come Bitcoin (BTC), Ethereum (ETH), Binance Coin (BNB), Dogecoin (DOGE), Solana (SOL) e Ripple (XRP). Per ciascuna di queste criptovalute, sono stati inclusi due tipi di dati:

**2. Materie Prime**:    Le commodities_columns raggruppano i dati relativi a una selezione di materie prime tradizionali, tra cui:
 - Cattle (bovini)
 - Corn (mais)
 - Crude oil (petrolio)
 - Gold (oro)
 - Silver (argento)
 - Soybeans (soia)
 - Wheat (grano)


**3. Indici Finanziari**:   Il gruppo indices_columns comprende una serie di indici finanziari che riflettono la performance complessiva dei mercati azionari globali. Gli indici inclusi nel dataset sono:
 - CAC (Francia)
 - DAX (Germania)
 - Dow Jones Industrial Average (USA)
 - EURO (Eurozona)
 - FTSE (Regno Unito)
 - BOVESPA (Brasile)
 - IPC (Messico)
 - NASDAQ (USA)
 - S&P (USA)

**4. Indicatori Macroeconomici e Trend**:   Il gruppo macro_and_trends_columns raccoglie vari indicatori economici e sociali che possono influenzare i mercati finanziari e le decisioni di investimento. Questo gruppo include:
 - Funding rate: il tasso di interesse applicato alle posizioni di margin trading nelle criptovalute, che può riflettere il livello di fiducia e la domanda nel mercato delle criptovalute.
 - Fear and Greed Index: un indicatore che misura l’emotività del mercato, utile per capire se gli investitori sono spinti dalla paura o dall’avidità.
 - Google Trends: indicatori basati sulle ricerche di Google relative a criptovalute e Bitcoin, che riflettono l’interesse pubblico verso questi asset digitali.
 - VIX (Volatility Index): uno degli indicatori più noti per misurare la volatilità del mercato azionario e l’incertezza economica.
"""

y = ['BTC_USDT_1h_close', 'BTC_USDT_1h_volume']

cryptos_columns = [
    'BTC_USDT_1h_close', 'BTC_USDT_1h_volume',
    'BNB_USDT_1h_close', 'BNB_USDT_1h_volume',
    'DOGE_USDT_1h_close', 'DOGE_USDT_1h_volume',
    'ETH_USDT_1h_close', 'ETH_USDT_1h_volume',
    'SOL_USDT_1h_close', 'SOL_USDT_1h_volume',
    'XRP_USDT_1h_close', 'XRP_USDT_1h_volume',
]

commodities_columns = [
    'BTC_USDT_1h_close', 'BTC_USDT_1h_volume',
    'cattle_Close LE=F', 'cattle_Volume LE=F',
    'corn_Close ZC=F', 'corn_Volume ZC=F',
    'crude_Close CL=F', 'crude_Volume CL=F',
    'gold_Close GC=F', 'gold_Volume GC=F',
    'silver_Close SI=F', 'silver_Volume SI=F',
    'soybeans_Close ZS=F', 'soybeans_Volume ZS=F',
    'wheat_Close ZW=F', 'wheat_Volume ZW=F'
]

indices_columns = [
    'BTC_USDT_1h_close', 'BTC_USDT_1h_volume',
    'CAC_Close ^FCHI', 'CAC_Volume ^FCHI',
    'DAX_Close ^GDAXI', 'DAX_Volume ^GDAXI',
    'Dow_Close ^DJI', 'Dow_Volume ^DJI',
    'EURO_Close ^STOXX50E', 'EURO_Volume ^STOXX50E',
    'FTSE_Close ^FTSE', 'FTSE_Volume ^FTSE',
    'IBOVESPA_Close ^BVSP', 'IBOVESPA_Volume ^BVSP',
    'IPC_Close ^MXX', 'IPC_Volume ^MXX',
    'NASDAQ_Close ^IXIC', 'NASDAQ_Volume ^IXIC',
    'Russell_Close ^RUT', 'Russell_Volume ^RUT',
    'S&P_Close ^GSPC', 'S&P_Volume ^GSPC',
    'S&P_Close ^GSPTSE', 'S&P_Volume ^GSPTSE',
]

macro_and_trends_columns = ['funding_rate', 'fear_gread_index', 'google_trends_buy_crypto', 'google_trends_bitcoin', 'BTC_USDT_1h_close', 'VIX_Close ^VIX']

def normalize_base_100(series):
    return (series / series.iloc[0]) * 100 if series.iloc[0] != 0 else series

def plot_group_vs_btc(data, group_columns, group_name):
    if group_name == 'Macro e Trends':
        fig = make_subplots(rows=1, cols=1, subplot_titles=[f"{group_name}"])

        for col in group_columns:
            fig.add_trace(go.Scatter(x=data['Datetime'],y=normalize_base_100(data[col]),mode='lines',name=col,
                line=dict(color=px.colors.qualitative.Plotly[len(fig.data) % len(px.colors.qualitative.Plotly)]), showlegend=True), row=1, col=1)

    else: # Per gli altri gruppi (cripto, commodities, indici), continuiamo con la logica di prezzi e volumi
        close_columns = [col for col in group_columns if 'close' in col.lower()]
        volume_columns = [col for col in group_columns if 'volume' in col.lower()]

        base_assets = list(set([col.split('_')[0] for col in close_columns])) # Estrai base names per accoppiare price-volume (es: 'ETH')
        color_palette = px.colors.qualitative.Plotly
        asset_colors = {asset: color_palette[i % len(color_palette)] for i, asset in enumerate(base_assets)}

        fig = make_subplots(rows=1, cols=2, subplot_titles=(f"{group_name} - Prezzi (Base 100)", f"{group_name} - Volumi (Base 100)"))
        # --- Prezzi (colonna 1) ---
        for col in close_columns:
            asset = col.split('_')[0]
            fig.add_trace(go.Scatter(x=data['Datetime'],y=normalize_base_100(data[col]),mode='lines',name=f"{asset}",
                                     line=dict(color=asset_colors[asset]),legendgroup=asset,showlegend=True  ), row=1, col=1)

        # --- Volumi (colonna 2) ---
        for col in volume_columns:
            asset = col.split('_')[0]
            fig.add_trace(go.Scatter(x=data['Datetime'],y=normalize_base_100(data[col]),mode='lines',name=f"{asset}",
                                     line=dict(color=asset_colors[asset], dash='solid'),legendgroup=asset,showlegend=False), row=1, col=2)

    fig.update_layout(
        title={'text': f"Andamento temporale di {group_name} con BTC (Prezzi e Volumi)", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        template='plotly_dark',height=600,width=1500,showlegend=True)
    fig.show()

#@title Analisi Bivariata tra andamento del prezzo e volumi del Bitcoin in base ai gruppi delle variabili
plot_group_vs_btc(data, cryptos_columns, "Cryptos")
plot_group_vs_btc(data, commodities_columns, "Commodities")
plot_group_vs_btc(data, indices_columns, "Indices")
plot_group_vs_btc(data, macro_and_trends_columns, "Macro e Trends")

#volume_columns = [col for col in data_merged.columns if 'volume' in col.lower()]
#df_volume = data_merged[['Datetime', 'funding_rate', 'fear_gread_index', 'google_trends_buy_crypto', 'google_trends_bitcoin'] + volume_columns]

close_columns = [col for col in data.columns if 'close' in col.lower()]
df_close_originale = data[['Datetime', 'funding_rate', 'fear_gread_index', 'google_trends_buy_crypto', 'google_trends_bitcoin'] + close_columns]

rename_map = {}

for col in df_close_originale.columns:
    if 'BTC' in col and 'close' in col: rename_map[col] = 'BTC'
    elif 'BNB' in col and 'close' in col: rename_map[col] = 'BNB'
    elif 'DOGE' in col and 'close' in col: rename_map[col] = 'DOGE'
    elif 'ETH' in col and 'close' in col: rename_map[col] = 'ETH'
    elif 'SOL' in col and 'close' in col: rename_map[col] = 'SOL'
    elif 'XRP' in col and 'close' in col: rename_map[col] = 'XRP'
    elif 'cattle' in col.lower(): rename_map[col] = 'cattle'
    elif 'corn' in col.lower(): rename_map[col] = 'corn'
    elif 'crude' in col.lower(): rename_map[col] = 'crude'
    elif 'gold' in col.lower(): rename_map[col] = 'gold'
    elif 'silver' in col.lower(): rename_map[col] = 'silver'
    elif 'soybeans' in col.lower(): rename_map[col] = 'soybeans'
    elif 'wheat' in col.lower(): rename_map[col] = 'wheat'
    elif 'CAC' in col: rename_map[col] = 'CAC'
    elif 'DAX' in col: rename_map[col] = 'DAX'
    elif 'Dow' in col: rename_map[col] = 'Dow'
    elif 'STOXX' in col: rename_map[col] = 'EURO'
    elif 'FTSE' in col: rename_map[col] = 'FTSE'
    elif 'BVSP' in col: rename_map[col] = 'IBOVESPA'
    elif 'MXX' in col: rename_map[col] = 'IPC'
    elif 'IXIC' in col: rename_map[col] = 'NASDAQ'
    elif 'RUT' in col: rename_map[col] = 'Russell'
    elif 'GSPC' in col and 'TSE' not in col: rename_map[col] = 'SP'
    elif 'GSPTSE' in col: rename_map[col] = 'SP_TSE'
    elif 'VIX' in col: rename_map[col] = 'VIX'
    elif col == 'funding_rate': rename_map[col] = 'funding rt'
    elif col == 'fear_gread_index': rename_map[col] = 'fear greed'
    elif col == 'google_trends_buy_crypto': rename_map[col] = 'G_Trends crypto'
    elif col == 'google_trends_bitcoin': rename_map[col] = 'G_Trends BTC'

data_close = df_close_originale.rename(columns=rename_map)
data_close.head(3)

cryptos = ['BTC', 'BNB', 'DOGE', 'ETH', 'SOL', 'XRP']
commodities = [#'BTC',
               'cattle', 'corn', 'crude', 'gold', 'silver', 'soybeans', 'wheat']
indices = [#'BTC',
           'CAC', 'DAX', 'Dow', 'EURO', 'FTSE', 'IBOVESPA', 'IPC', 'NASDAQ', 'Russell', 'SP', 'SP_TSE']
macro_and_trends = [#'BTC',
                    'funding rt', 'fear greed', 'G_Trends crypto', 'G_Trends BTC', 'VIX']

#@title Corr Matrix singole
def plot_correlation_matrix(df, variables_group, group_name):
    df_corr = df[[col for col in df.columns if col in variables_group]]
    corr_matrix = df_corr.corr()

    fig = px.imshow(corr_matrix,text_auto=".2f",color_continuous_scale='RdBu',
        labels=dict(x="Features", y="Features", color="Correlation"),
    )

    fig.update_layout(title={'text': f"Matrice di Correlazione: {group_name} e BTC", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        height=600,width=600,showlegend=True)
    fig.show()

plot_correlation_matrix(data_close, cryptos, "Cryptos")
plot_correlation_matrix(data_close, commodities, "Commodities")
plot_correlation_matrix(data_close, indices, "Indices")
plot_correlation_matrix(data_close, macro_and_trends, "Macro e Trends")

#@title Correlation Matrix, subplot per gruppi di variabili
def plot_correlation_subplots(df, groups_dict):
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[f"{name} e BTC" for name in groups_dict.keys()],
        horizontal_spacing=0.1,
        vertical_spacing=0.1
    )

    row_col_positions = [(1, 1), (1, 2), (2, 1), (2, 2)]

    for (group_name, variables), (row, col) in zip(groups_dict.items(), row_col_positions):
        df_corr = df[[col for col in df.columns if col in variables]]
        corr_matrix = df_corr.corr()

        heatmap = go.Heatmap(
            z=corr_matrix.values,
            x=corr_matrix.columns,
            y=corr_matrix.columns,
            colorscale='RdBu',
            zmin=-1, zmax=1,
            colorbar=dict(title="Correlation"),
            text=corr_matrix.round(2).values,
            texttemplate="%{text}",
        )

        fig.add_trace(heatmap, row=row, col=col)

    fig.update_layout(title={'text': f"Matrice di Correlazione tra i gruppi delle variabili", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        height=1200,width=1200,showlegend=True)
    fig.show()

groups = {
    "Cryptos": cryptos,
    "Commodities": commodities,
    "Indices": indices,
    "Macro e Trends": macro_and_trends
}

plot_correlation_subplots(data_close, groups)

corr_matrix = data_close.corr()

fig_cm = ff.create_annotated_heatmap(z=corr_matrix.values, annotation_text=np.around(corr_matrix.values, decimals=2),x=corr_matrix.columns.tolist(),y=corr_matrix.columns.tolist(),
                                     showscale=True, colorscale='RdBu',reversescale=False,hoverinfo='text',)
fig_cm.update_layout(title={'text': f"Matrice di Correlazione", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        height=800,width=1500,showlegend=True)
fig_cm.show()

"""### 1.3. Analisi dei rendimenti"""

data_close.info()

periods_dict = {'1d': 1*7, '1w': 7*24, '1m': 30*24, '6m': 6*30*24, '1y': 365*24}
commodities = ['soybeans', 'gold', 'corn', 'wheat']
indici = ['IPC', 'Dow', 'SP', 'EURO', 'NASDAQ']
crypto = ['BTC']
columns_to_analyze = commodities + indici + crypto

data_rendimenti = pd.DataFrame()
data_rendimenti['Datetime'] = data_close['Datetime']

def calcola_rendimento(df, colonna, periodi_dict):
    rendimenti = {}
    for periodo, delta in periods_dict.items():
        rendimenti[periodo] = df[colonna].pct_change(periods=delta)  # Calcolo del rendimento percentuale
    return rendimenti
for colonna in columns_to_analyze:
    rendimenti = calcola_rendimento(data_close, colonna, periods_dict)

    # Aggiungere i rendimenti calcolati nel dataframe finale
    for periodo, rendimento in rendimenti.items():
        data_rendimenti[f'{colonna}_{periodo}'] = rendimento

data_rendimenti.info()

fig = go.Figure()

fig.add_trace(go.Scatter(x=data_rendimenti['Datetime'], y=data_rendimenti['BTC_1d'], mode='lines', name='1 Giorno', line=dict(width=1)))
fig.add_trace(go.Scatter(x=data_rendimenti['Datetime'], y=data_rendimenti['BTC_1w'], mode='lines', name='1 Settimana', line=dict(width=1)))
fig.add_trace(go.Scatter(x=data_rendimenti['Datetime'], y=data_rendimenti['BTC_1m'], mode='lines', name='1 Mese', line=dict(width=1)))
fig.add_trace(go.Scatter(x=data_rendimenti['Datetime'], y=data_rendimenti['BTC_6m'], mode='lines', name='6 Mesi', line=dict(width=1)))
fig.add_trace(go.Scatter(x=data_rendimenti['Datetime'], y=data_rendimenti['BTC_1y'], mode='lines', name='1 Anno', line=dict(width=1)))

fig.update_layout(
    title={'text': f"Rendimenti BTC su diversi periodi", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    height=500,width=1200,showlegend=True, xaxis_title='Data', yaxis_title='Rendimento (%)', template='plotly_dark')

fig.show()

def plot_rendimenti(data_rendimenti, columns, periodo, title="Rendimenti", xaxis_title='Data', yaxis_title='Rendimento (%)'):
    fig = go.Figure()

    for col in columns:
        col_name = f"{col}_{periodo}"
        if col_name in data_rendimenti.columns:
            data_rendimenti_non_na = data_rendimenti[['Datetime', col_name]].dropna()
            fig.add_trace(go.Scatter(x=data_rendimenti_non_na['Datetime'], y=data_rendimenti_non_na[f'{col}_{periodo}'],
                mode='lines', name=col, line=dict(width=1)))

    fig.update_layout(
        title={'text': f"{title} a {periodo}", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        height=500, width=1200, showlegend=True, xaxis_title=xaxis_title, yaxis_title=yaxis_title, template='plotly_dark')

    fig.show()

plot_rendimenti(data_rendimenti, ['BTC', 'IPC', 'Dow', 'SP', 'EURO', 'NASDAQ'], '6m', title="Rendimenti BTC e principali Indici", xaxis_title='Data', yaxis_title='Rendimento (%)')
plot_rendimenti(data_rendimenti, ['BTC', 'IPC', 'Dow', 'SP', 'EURO', 'NASDAQ'], '1y', title="Rendimenti BTC e principali Indici", xaxis_title='Data', yaxis_title='Rendimento (%)')

plot_rendimenti(data_rendimenti, ['BTC','soybeans', 'gold', 'corn', 'wheat'], '6m', title="Rendimenti BTC e principali Commodities", xaxis_title='Data', yaxis_title='Rendimento (%)')
plot_rendimenti(data_rendimenti, ['BTC','soybeans', 'gold', 'corn', 'wheat'], '1y', title="Rendimenti BTC e principali Commodities", xaxis_title='Data', yaxis_title='Rendimento (%)')

"""## 2. Analsi Predittiva: Market Trend

### 2.1. Preparazione del daset per l'analisi
"""

columns_to_keep_lstm = []
for col in data_merged.columns:
    col_lower = col.lower()
    if any(key in col_lower for key in ['open', 'high', 'low', 'volume']) and 'close' not in col_lower:
        continue
    columns_to_keep_lstm.append(col)

df_lstm_with_dt = data_merged[columns_to_keep_lstm]
print("Colonne mantenute in df_lstm_with_dt:")
print(df_lstm_with_dt.columns.tolist())

df_lstm_with_dt['Datetime'] = pd.to_datetime(df_lstm_with_dt['Datetime'])

df_lstm = df_lstm_with_dt.drop(columns=['Datetime'])
df_lstm.head()

df_lstm.info()

X_lstm = df_lstm.drop(columns=['BTC_USDT_1h_close'])
#ANALISI VIF
X_lstm_const = sm.add_constant(X_lstm)

# Calcolo VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X_lstm.columns
vif_data["VIF"] = [variance_inflation_factor(X_lstm_const.values, i+1) for i in range(len(X_lstm.columns))]  # i+1 per saltare la costante

print(vif_data)

data = {
    "feature": ["BNB_USDT_1h_close", "DOGE_USDT_1h_close", "ETH_USDT_1h_close", "SOL_USDT_1h_close", "XRP_USDT_1h_close",
                "cattle_Close LE=F", "corn_Close ZC=F", "crude_Close CL=F", "gold_Close GC=F", "silver_Close SI=F",
                "soybeans_Close ZS=F", "wheat_Close ZW=F", "CAC_Close ^FCHI", "DAX_Close ^GDAXI", "Dow_Close ^DJI",
                "EURO_Close ^STOXX50E", "FTSE_Close ^FTSE", "IBOVESPA_Close ^BVSP", "IPC_Close ^MXX",
                "NASDAQ_Close ^IXIC", "Russell_Close ^RUT", "S&P_Close ^GSPC", "S&P_Close ^GSPTSE", "VIX_Close ^VIX",
                "funding_rate", "fear_gread_index", "google_trends_buy_crypto", "google_trends_bitcoin"],
    "VIF": [67.22, 28.26, 27.56, 48.86, 27.88, 6.87, 14.44, 5.64, 138.34, 39.91, 22.42, 5.01, 64.02, 229.64, 325.37,
            332.86, 22.40, 10.50, 9.05, 792.54, 33.04, 2223.85, 142.44, 5.87, 12.57, 4.46, 9.50, 6.06]
}

df = pd.DataFrame(data)

def assign_color(vif):
    if vif < 5:
        return 'green'
    elif 5 <= vif <= 10:
        return 'orange'
    else:
        return 'red'

df['color'] = df['VIF'].apply(assign_color)

fig_vif = px.bar(df, x="feature", y="VIF", color="color",color_discrete_map={"green": "green", "orange": "orange", "red": "red"},title="VIF Histogram con Colorazione per Soglie")
fig_vif.update_layout(xaxis_tickangle=-45, width=1200, height=600)
fig_vif.show()

"""#### Correlazione Features con Target"""

abbreviated_columns = [
    'bnb_close', 'doge_close', 'eth_close', 'sol_close', 'xrp_close',
    'cattle_close', 'corn_close', 'crude_close', 'gold_close', 'silver_close',
    'soybeans_close', 'wheat_close',
    'cac_close', 'dax_close', 'dow_close', 'euro_close', 'ftse_close',
    'ibov_close', 'ipc_close', 'nasdaq_close', 'russell_close', 'sp_close', 'tsx_close', 'vix_close',
    'funding_rate', 'fear_greed', 'g_trend_buy_crypto', 'g_trend_bitcoin'
]

# Calcolo della matrice di correlazione
corr_matrix = df_lstm.corr()
target_corr = corr_matrix['BTC_USDT_1h_close'].drop('BTC_USDT_1h_close')
target_corr_sorted = target_corr.reindex(target_corr.abs().sort_values(ascending=False).index)

corr_df = pd.DataFrame({
    'Variabile': target_corr_sorted.index,
    'Correlazione': target_corr_sorted.values,
    'Variabile Abbreviata': abbreviated_columns
})


fig = px.bar(corr_df,x='Correlazione',y='Variabile',
    orientation='h',
    color='Correlazione', color_continuous_scale='RdBu')

fig.update_layout(title={'text': "Correlazione delle feature con BTC_USDT_1h_close",'x': 0.5,
                         'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},},
                  yaxis=dict(autorange="reversed"),  height=800, width=1200, )
fig.show()

fig.write_image('Correlazione_target_features.pdf')

significant_features = target_corr[abs(target_corr) > 0.1].index.tolist()
print("Variabili significative:", significant_features)

"""Il grafico mostra una forte
correlazione positiva di
Bitcoin con molti asset
azionari ed altre
cryptovalute. Tutte le variabili del
dataset sono inizialmente
considerabili come
significative
(|corr_index|>0.1)

#### Correlazione tra le Features
"""

features = df_lstm.drop(columns=['BTC_USDT_1h_close'])
correlation_matrix = features.corr()
original_columns = list(correlation_matrix.columns)

fig_cm = ff.create_annotated_heatmap(z=correlation_matrix.values, x=abbreviated_columns, y=abbreviated_columns,
    annotation_text=np.around(correlation_matrix.values, decimals=2),showscale=True, colorscale='RdBu',
    reversescale=True,
    hoverinfo='text',
)

fig_cm.update_layout(
    title={
        'text': "Correlation Matrix",
        'x': 0.5,
        'xanchor': 'center',  # Ancoraggio al centro
        'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},
    },
    margin=dict(t=200, b=0, l=0, r=0),
    height=800,
    width=1500,
)
fig_cm.show()

"""#### PCA

Data la multicollinearità presente tra le varie feature (osservata nella
matrice di correlazione tra le variabili), si è proceduto nella
realizzazione di un PCA (Principal Component Analysis) per ridurre il
numero di feature utilizzate nel modello.
"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

pca = PCA() #PCA senza ridurre il numero di componenti
X_pca = pca.fit(X_scaled)
explained_variance = X_pca.explained_variance_ratio_ #calcolo la varianza spiegata da ciascuna componente

# Numero di componenti che spiegano la % della varianza
percentuale_var = 0.95
cumulative_variance = X_pca.explained_variance_ratio_.cumsum()
n_components = (cumulative_variance >= percentuale_var).argmax() + 1
print(f"Numero di componenti che spiegano il {percentuale_var} della varianza: {n_components}")

fig_pca = go.Figure()

# Aggiungiamo la traccia per la "Varianza Spiegata" (sinistra)
fig_pca.add_trace(go.Scatter(x=list(range(1, len(explained_variance) + 1)), y=explained_variance,
                             mode='lines+markers', marker=dict(color='#636EFA'), name='Varianza Spiegata',
                             hovertemplate='%{x}<br>Varianza Spiegata: %{y:.4f}<extra></extra>'))

# Aggiungiamo la traccia per la "Varianza Cumulativa" (destra)
fig_pca.add_trace(go.Scatter(x=list(range(1, len(explained_variance) + 1)), y=explained_variance.cumsum(),
                             mode='lines+markers', marker=dict(color='#EF553B'), name='Varianza Cumulativa',
                             hovertemplate='%{x}<br>Varianza Cumulativa: %{y:.4f}<extra></extra>',
                             yaxis="y2"))

# Linea orizzontale a 95% sulla y2
y_value_percent = percentuale_var * explained_variance.sum()  # 95% del totale della varianza cumulativa
fig_pca.add_trace(go.Scatter(x=[1, len(explained_variance)], y=[y_value_percent, y_value_percent],
                             mode='lines', line=dict(color='gray', dash='dash', width=1), name=f'Linea {round(y_value_percent,4)*100}% Varianza Cumulativa'))

#punto di intersezione tra la curva di varianza cumulativa e la linea orizzontale
cumulative_variance = explained_variance.cumsum()
x_intersection = np.argmax(cumulative_variance >= y_value_percent) + 1  # Aggiungiamo 1 perché gli indici partono da 0
y_intersection = cumulative_variance[x_intersection - 1]  # Otteniamo il valore della varianza cumulativa al punto di intersezione

#Linea verticale perpendicolare (passante per il punto di intersezione)
fig_pca.add_trace(go.Scatter(x=[x_intersection, x_intersection], y=[0, y_intersection],
                             mode='lines', line=dict(color='gray', dash='dash', width=1), name='Linea Perpendicolare'))

fig_pca.update_layout(
    title={'text': "Analisi PCA - Varianza per Componente", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    showlegend=True, width=1200, height=600,
    yaxis=dict(title="Varianza Spiegata", titlefont=dict(color="#636EFA")),
    yaxis2=dict(title="Varianza Cumulativa", titlefont=dict(color="#EF553B"), overlaying="y", side="right"),
    xaxis=dict(title="Numero di Componente",dtick=1, tickvals=list(range(1, len(explained_variance) + 1)) ),
    legend=dict(x=1.05, xanchor='left', yanchor='middle')
)

fig_pca.show()

components = X_pca.components_[:n_components] # Prendi i carichi delle prime 'n_components' componenti
var_names = features.columns
pesi_var_pca = pd.DataFrame(components, columns=var_names) #DataFrame con i carichi delle variabili
pesi_var_pca.round(4)
print("Carichi delle variabili per le componenti selezionate:")
pesi_var_pca

#Le variabili più significative per ogni componente
for i in range(n_components):
    top_features = pesi_var_pca.iloc[i].abs().sort_values(ascending=False).head(5)
    print(f"\nLe variabili più significative per la componente {i+1}:")
    print(top_features)

X_pca = pca.transform(X_scaled)  # X_scaled è il tuo dataset normalizzato

# Crea un DataFrame con le componenti principali
pca_df = pd.DataFrame(X_pca[:, :n_components], columns=[f'PC{i+1}' for i in range(n_components)])
pca_df['BTC_USDT_1h_close'] = df_lstm['BTC_USDT_1h_close'].values
pca_df.head()

pca_df.info()

X_pca = pca_df.drop(columns=['BTC_USDT_1h_close'])
#ANALISI VIF
X_pca_const = sm.add_constant(X_pca)

# Calcolo VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X_pca.columns
vif_data["VIF"] = [variance_inflation_factor(X_pca_const.values, i+1) for i in range(len(X_pca.columns))]  # i+1 per saltare la costante

print(vif_data)

#Verifica se ha mantenuto l'ordine delle righe
print(df_lstm['BTC_USDT_1h_close'].head(-5))
print(pca_df['BTC_USDT_1h_close'].head(-5))

"""### 2.2. Train e Test"""

df_ridotto = pca_df

giorno = 24
settimana = 24 * 7
mese = 24 * 30
sei_mesi = 24 * 30 * 6
anno = 24 * 30 *12
due_anni =24 * 30 *12 *2

#Definizione del Train e Test Set
train_size = int(len(df_ridotto) * 0.8) #dimensione del train. Prendiamo l'80% del dataframe
train, test = df_ridotto.iloc[:train_size], df_ridotto.iloc[train_size:] #per il train prendiamo dal df, dal primo valore fino all'ultimo valore di train_size.
#                                                                         Mentre per il test, prendiamo dal valore di tran_size fino alla fine (20%)

#per avere come indice la data, così da metterla nell'asse X del plot
#ricava dal dataset il datatime, usando le posizioni per creare il test e train dates
train_dates = df_lstm_with_dt['Datetime'].iloc[:train_size]
test_dates = df_lstm_with_dt['Datetime'].iloc[train_size:]

#Plot del train e test set
def plot_time_series_plotly(train, test, train_dates, test_dates, title="Time Series: divisione del dataframe in train e test"):
    fig = go.Figure()
    spessore = 2
    fig.add_trace(go.Scatter(x=train_dates, y=train['BTC_USDT_1h_close'],mode='lines',name='Train', line=dict(width=spessore),
                             hovertemplate='%{x|%Y-%m-%d}<br>Prezzo: %{y:.2f}<extra></extra>'))
    fig.add_trace(go.Scatter(x=test_dates, y=test['BTC_USDT_1h_close'],mode='lines',name='Test', line=dict(width=spessore),
                             hovertemplate='%{x|%Y-%m-%d}<br>Prezzo: %{y:.2f}<extra></extra>'))
    fig.update_layout(title={'text': title,'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},},
                      xaxis_title='Date',yaxis_title='Price',
                      width=1200,height=700)
    fig.show()

plot_time_series_plotly(train[['BTC_USDT_1h_close']],test[['BTC_USDT_1h_close']],train_dates,test_dates)

"""Abbiamo preparato il test e train set per un'analisi predittiva (timeseries) con una previsione di una settimana"""

target_column = 'BTC_USDT_1h_close'
sequence_length = settimana  # una settimana se dati orari
features = [col for col in df_ridotto.columns if col != target_column]

train_size = int(len(df_ridotto) * 0.8)
train = df_ridotto.iloc[:train_size]
test = df_ridotto.iloc[train_size:]

scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train[features + [target_column]])
test_scaled = scaler.transform(test[features + [target_column]])

def create_sequences_multivariate(data, sequence_length, target_column_index):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i + sequence_length, :-1])  # tutte le feature tranne la target
        y.append(data[i + sequence_length, target_column_index])  # target alla fine
    return np.array(X), np.array(y)

target_index = len(features)

X_train, y_train = create_sequences_multivariate(train_scaled, sequence_length, target_index)
X_test, y_test = create_sequences_multivariate(test_scaled, sequence_length, target_index)

print("--- Controllo finale ---")
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""Non da usare

```
# Funzione per creare una sequenza per LSTM - Analisi Monovariata
def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length])
    return np.array(X), np.array(y) #trasformiamo in np array

scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train[['BTC_USDT_1h_close']])
test_scaled = scaler.transform(test[['BTC_USDT_1h_close']])

#Lunghezza della predizione
sequence_length = settimana

# Creazione delle sequenza con la funzione
X_train, y_train = create_sequences(train_scaled, sequence_length)
X_test, y_test = create_sequences(test_scaled, sequence_length)

# Reshape input to be [samples, time steps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
```


"""

X_train.shape

X_train.shape
"""
X_train.shape = (13844, 168, 28) significa che ora stai correttamente usando:
	•	13844 campioni (finestre temporali)
	•	168 timestep (quindi 1 settimana di dati orari)
	•	28 feature per ogni timestep (quindi 28 colonne usate come input)
    """

lunghezza_sequenz = X_train.shape[1]
n_features = X_train.shape[2]
print(f"Lunghezza della sequenza: {lunghezza_sequenz}")
print(f"Numero di feature: {n_features}")

"""### 2.3. Modello LSTM - Autoregressivo

Tra le varie opzioni di modelli di machine learning o reti neurali, la
scelta è ricaduta su Long Short-Term Memory layer model (LSTM). Tale
decisione è stata effettuata in quanto i dati a disposizione sono di tipo
temporale e non sarebbe stato accurato utilizzare altri modelli di
Machine Learning.
I dati utilizzati saranno esclusivamente i dati di mercato «close» per
evitare inutili ridondanze.


Questo modello  fa previsioni su base oraria, e per fare ogni previsione ha bisogno di una “finestra” di sequence_length = 168 ore precedenti (1 settimana). Quindi  il modello  non produce previsioni per i primi sequence_length, perché ha bisogno di quella finestra come input per la prima previsione.

Ad esempio:
- Se sequence_length = 168, la prima previsione sarà in corrispondenza del dato 168.
- Quindi i primi 168 valori reali non vanno confrontati con nulla, perché non c’è ancora una previsione.


"""

"""
# Define custom metrics
def R2Score(y_true, y_pred):
    SS_res =  tf.reduce_sum(tf.square( y_true-y_pred ))
    SS_tot = tf.reduce_sum(tf.square( y_true - tf.reduce_mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + tf.keras.backend.epsilon()) )

def RootMeanSquaredError(y_true, y_pred):
    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))


# Load the model with custom objects
model = load_model("/content/modello_lstm_completo.keras",
                   custom_objects={'R2Score': R2Score,
                                   'RootMeanSquaredError': RootMeanSquaredError})
"""
model = load_model("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/modello_completo.keras")
with open("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/history.pkl", "rb") as file_pi:
    history = pickle.load(file_pi)

y_pred_scaled = model.predict(X_test)
y_test_reshaped = np.zeros((y_test.shape[0], train_scaled.shape[1]))
y_test_reshaped[:, target_index] = y_test

y_test_actual = scaler.inverse_transform(y_test_reshaped)[:, target_index]

y_pred_reshaped = np.zeros((y_pred_scaled.shape[0], train_scaled.shape[1]))
y_pred_reshaped[:, target_index] = y_pred_scaled.flatten()
y_pred = scaler.inverse_transform(y_pred_reshaped)[:, target_index]

lstm_mse = mean_squared_error(y_test_actual, y_pred)
lstm_mae = mean_absolute_error(y_test_actual, y_pred)
lstm_r2 = r2_score(y_test_actual, y_pred)
lstm_loss = model.evaluate(X_test, y_test)

"""#### Modello"""

tensorflow.keras.backend.clear_session()

# Definizione del modello
tensorflow.random.set_seed(42)

model = Sequential()

Reg_l1 = 0.00
Reg_l2 = 0.00
DP_out = 0.5

#Input Layer
model.add(Input(shape=(sequence_length, n_features)))  # Input esplicito

#Hidden Layers
#model.add(LSTM(64, activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=False))
#model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=Reg_l1, l2=Reg_l2)))
#model.add(Dropout(DP_out))
model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=Reg_l1, l2=Reg_l2)))
#model.add(Dropout(0.3))
model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=Reg_l1, l2=Reg_l2)))

#Output Layers
model.add(Dense(1, kernel_regularizer=l1_l2(l1=Reg_l1, l2=Reg_l2)))

# Compilazione
model.compile(optimizer='adam',
              loss='mse',
              metrics=[tensorflow.keras.metrics.R2Score(), tensorflow.keras.metrics.RootMeanSquaredError()])

model.summary()

"""Per l’addestramento del
modello è stato inserito un
EarlyStopg in caso la
variazione marginale della
value loss sia inferiore a
0,00001, fino ad un massimo di
ducecento epoche e una
batch_size di trenta.
"""

early_stop = EarlyStopping(monitor='val_loss',min_delta=1e-5,patience=5,verbose=1,mode='auto',restore_best_weights=True)
epoche = 50
n_batch_size = 30

print(f"I casi presenti nel dataset:{len(df_lstm-1)}, \n essendo che la batch size è di {n_batch_size}, serviranno {len(df_lstm-1)/n_batch_size} passaggi per completare un'epoca ")

history = model.fit(X_train, y_train,
                    epochs=epoche, batch_size=n_batch_size,
                    verbose=1, validation_split=0.1,
                    callbacks=[early_stop
                               ])

# Salva modello e history
model.save("modello_completo.keras")
with open("history.pkl", "wb") as file_pi:
    pickle.dump(history.history, file_pi)

# === In futuro ===
"""
# Carica modello e history
from tensorflow.keras.models import load_model
import pickle

model = load_model("modello_completo.h5")
with open("history.pkl", "rb") as file_pi:
    loaded_history = pickle.load(file_pi)
"""

"""Spiegazione codice:

```
y_pred_scaled = model.predict(X_test)
```
- model.predict(X_test) esegue la predizione sul set di test (X_test) usando il modello LSTM già addestrato.
- Il risultato (y_pred_scaled) è scalato, cioè normalizzato (per esempio tra 0 e 1).



```
y_test_reshaped = np.zeros((y_test.shape[0], train_scaled.shape[1]))  
y_test_reshaped[:, target_index] = y_test  
```
- y_test contiene i valori reali da confrontare con le predizioni.
- Viene creato un array pieno di zeri della stessa lunghezza dei dati di test, ma con tutte le colonne presenti nel set scalato originale (train_scaled.shape[1]).
- Si riempie solo la colonna dell’indice target (target_index) con i valori reali (y_test), lasciando le altre colonne a 0. Questo serve per poter usare lo scaler inverso, che si aspetta un array con tutte le feature originali.



```
y_test_actual = scaler.inverse_transform(y_test_reshaped)[:, target_index]
```
- Si esegue la trasformazione inversa dello scaler per riportare i dati nella scala originale.
- Poi si estrae solo la colonna (target_index), che è il vero valore reale nella scala originale.




```
y_pred_reshaped = np.zeros((y_pred_scaled.shape[0], train_scaled.shape[1]))
y_pred_reshaped[:, target_index] = y_pred_scaled.flatten()  
y_pred = scaler.inverse_transform(y_pred_reshaped)[:, target_index]
```
Anche le predizioni vengono invertite nella scala originale, e si estrae solo la colonna di interesse.





"""

y_pred_scaled = model.predict(X_test) #y scalato da prima, con tecnica MinMax

# Ricostruzione del formato per l’inverso dello scaling
y_test_reshaped = np.zeros((y_test.shape[0], train_scaled.shape[1]))
y_test_reshaped[:, target_index] = y_test

y_test_actual = scaler.inverse_transform(y_test_reshaped)[:, target_index]

y_pred_reshaped = np.zeros((y_pred_scaled.shape[0], train_scaled.shape[1]))
y_pred_reshaped[:, target_index] = y_pred_scaled.flatten()
y_pred = scaler.inverse_transform(y_pred_reshaped)[:, target_index]


lstm_mse = mean_squared_error(y_test_actual, y_pred)
lstm_mae = mean_absolute_error(y_test_actual, y_pred)
lstm_r2 = r2_score(y_test_actual, y_pred)
lstm_loss = model.evaluate(X_test, y_test)

print(f"LSTM Forecast - MSE: {lstm_mse}, MAE: {lstm_mae}, R2: {lstm_r2}, Loss: {lstm_loss}")

"""#### Grafici output"""

#@title Loss, MSE, e R² - Test vs Validation
#Per modelli caricati usare i primi (history), per modelli addestrati sul momemnto (history.history)

loss = history['loss']
val_loss = history['val_loss']
r2 = history['r2_score']
val_r2_score = history['val_r2_score']
root_mean_squared_error = history['root_mean_squared_error']
val_root_mean_squared_error = history['val_root_mean_squared_error']
"""

loss = history.history['loss']
val_loss = history.history['val_loss']
r2 = history.history['r2_score']
val_r2_score = history.history['val_r2_score']
root_mean_squared_error = history.history['root_mean_squared_error']
val_root_mean_squared_error = history.history['val_root_mean_squared_error']
"""

fig_lstm = make_subplots(rows=2, cols=2,vertical_spacing=0.15,subplot_titles=("Loss and Validation Loss", "MSE", "R2"), row_heights=[0.45, 0.45])
fig_lstm.add_trace(go.Scatter(x=list(range(len(loss))), y=loss, mode='lines', name='Loss'),row=1, col=1)
fig_lstm.add_trace(go.Scatter(x=list(range(len(val_loss))), y=val_loss, mode='lines', name='Validation Loss'),row=1, col=1)
fig_lstm.add_trace(go.Scatter(x=list(range(len(r2))), y=r2, mode='lines', name='R2'),row=2, col=1)
fig_lstm.add_trace(go.Scatter(x=list(range(len(val_r2_score))), y=val_r2_score, mode='lines', name='Validation R2'),row=2, col=1)
fig_lstm.add_trace(go.Scatter(x=list(range(len(root_mean_squared_error))), y=root_mean_squared_error, mode='lines', name='Root Mean Squared Error'),row=1, col=2)
fig_lstm.add_trace(go.Scatter(x=list(range(len(val_root_mean_squared_error))), y=val_root_mean_squared_error, mode='lines', name='Val Root Mean Squared Error'),row=1, col=2)

fig_lstm.update_layout(
    title= {'text': "Confronto Metriche di Performance: Loss, MSE, e R² - Test vs Validation",'x': 0.5, 'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Epochs",showlegend=True,width=1500,height=800,template='plotly_dark' )
fig_lstm.show()

"""Nonostante i valori eccellenti sul test set, sul validation set i valori di Loss, MSE e R2 prensenta risultati peggiori, il che suggerisce la presenza di overfitting.
Inoltre,tra la 15esima e 20esima epoca il modello tende a peggiorare.
"""

def plot_forecasts_plotly(test, actual_col, pred, test_dates, titolo):
    index = test_dates[sequence_length:] #Estrae le date corrispondenti alle previsioni. Salta le prime sequence_length date, perché non hai previsioni per quei giorni.
    actual = test[actual_col].values[sequence_length:] #Estrae i valori reali dal test, nella colonna 'BTC_USDT_1h_close'. Anche qui, salta i primi sequence_length dati, per restare allineato con le previsioni.

    fig = go.Figure()

    fig.add_trace(go.Scatter(x=index, y=actual, mode='lines', name='Actual', line=dict(color='white')))
    fig.add_trace(go.Scatter(x=index, y=pred, mode='lines', name='Forecast', line=dict(color='lightsalmon')))

    fig.update_layout(
        title= {'text': titolo, 'x': 0.5, 'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        xaxis_title="Date",
        yaxis_title="Price",
        legend=dict(x=0, y=1),
        template='plotly_dark',
        width=1200,
        height=700
    )
    fig.show()

plot_forecasts_plotly(test, 'BTC_USDT_1h_close', y_pred, test_dates, 'Forecast Comparison')

scarto = np.abs(y_test_actual - y_pred)
media_scarto = np.mean(scarto)
print("Lo scarto medio tra la Y reale e la predetta è di:",round(media_scarto, 2), "dollari")

y_corretta = y_pred + media_scarto
print("\nLa nuova y_corretta:", y_corretta, "\n\nLa y_pred:", y_pred)

def plot_forecasts_plotly(test, actual_col, pred, test_dates, titolo, sequence_length):
    index = test_dates[sequence_length:]
    actual = test[actual_col].values[sequence_length:]

    fig = go.Figure()

    fig.add_trace(go.Scatter(x=index, y=actual, mode='lines', name='Actual', line=dict(color='white')))
    fig.add_trace(go.Scatter(x=index, y=pred, mode='lines', name='Forecast_corretta', line=dict(color='deepskyblue')))

    fig.update_layout(
        title={'text': titolo, 'x': 0.5, 'xanchor': 'center','font': {'size': 20, 'family': 'Arial'}},
        xaxis_title="Date",
        yaxis_title="Price",
        legend=dict(x=0, y=1),
        template='plotly_dark',
        width=1200,
        height=700
    )

    return fig

fig = plot_forecasts_plotly(test, 'BTC_USDT_1h_close', y_corretta, test_dates, 'Forecast Comparison, con y_corretta',sequence_length)
fig.show()

#@title Previsioni nell'ultima settimana
#Le previsioni (y_corretta) iniziano da t = 168 in poi, in quanto le prime 168 ore non hanno previsioni (servono solo come input)
#quindi bisogna SFASARLI, se no i dati reali e predetti non sarebbero allineati
start_idx_test = -(2 * sequence_length) # = -336, prendi le ultime 336 ore (2 settimane) dai dati reali.
start_idx_y = -(1 * sequence_length) # = -168, prendi solo l’ultima settimana di previsioni.

#Ultime 336 ore: [---INPUT---][-----------PREVISIONI----------]
#                (168 ore)    (168 previsioni e dati reali)

test_week = test.iloc[start_idx_test:]
test_dates_week = test_dates[start_idx_test:]
y_corretta_week = y_corretta[start_idx_y:]


plot_forecasts_plotly(test_week, 'BTC_USDT_1h_close', y_corretta_week, test_dates_week, "Forecast Focus: nell'ultima settimana con y_corretta")

fig_learning_curve.write_image("Learning_Curve_GB_RedM.pdf")

"""Con la correzzione del target (y_corretta), il modello predice abbastanza bene l'andamento del prezzo di chiusura del Bitcoin. In dettaglio da quest'ultimo grafico, che fa un focus sulle prime 100 ore, il modello predice bene la tendenza del prezzo."""

def plot_r2_scatter(test, actual_col, pred, test_dates):
    index = test_dates[sequence_length:]
    actual = test[actual_col].values[sequence_length:]

    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=actual,y=pred, mode='markers', name='Predizioni',marker=dict(size=6, opacity=0.6, color='lightsalmon'))) #punti
    fig.add_trace(go.Scatter( x=[min_val, max_val], y=[min_val, max_val], mode='lines',name='y = x',line=dict(color='white', dash='dash'))) # Linea ideale: predicted = actual
    fig.update_layout(title={'text': f'R2 Scatter Plot: {round(r2_score(y_test_actual, pred),2)}', 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},xaxis_title='Actual',yaxis_title='Predicted',
        width=600,height=600,legend=dict(x=0, y=1),template='plotly_dark')
    fig.show()

#plot_r2_scatter(test, 'BTC_USDT_1h_close', y_pred, test_dates)
#plot_r2_scatter(test, 'BTC_USDT_1h_close', y_corretta, test_dates)

def plot_r2_subplots(test, actual_col, preds, labels, test_dates):
    index = test_dates[sequence_length:]
    actual = test[actual_col].values[sequence_length:]
    colors = ['lightsalmon', 'deepskyblue', 'lightgreen', 'gold', 'orchid', 'lightcoral']

    fig = make_subplots(rows=1, cols=len(preds),subplot_titles=[f"{label} (R²: {round(r2_score(actual, pred), 2)})" for label, pred in zip(labels, preds)])

    for i, (pred, label) in enumerate(zip(preds, labels)):
        min_val = min(actual.min(), pred.min())
        max_val = max(actual.max(), pred.max())
        show_legend = (i == 0)  # per non avere una doppia leggenda per la linea

        #punti
        fig.add_trace(go.Scatter(x=actual,y=pred,mode='markers', name=f'{label} - Predizioni', marker=dict(size=6, opacity=0.6, color=colors[i % len(colors)]),
            showlegend=True), row=1, col=i+1)

        # Linea y = x
        fig.add_trace(go.Scatter(x=[min_val, max_val],y=[min_val, max_val],mode='lines',name='y = x',line=dict(color='white', dash='dash'),showlegend=show_legend), row=1, col=i+1)

        fig.update_xaxes(title_text="Actual", row=1, col=i+1)
        fig.update_yaxes(title_text="Predicted", row=1, col=i+1)

    fig.update_layout(title={'text': 'Confronto Scatter Plot R² tra modelli','x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        width=1200,height=600,legend=dict(x=0, y=1),template='plotly_dark')

    fig.show()

plot_r2_subplots(
    test=test,
    actual_col='BTC_USDT_1h_close',
    preds=[y_pred, y_corretta],
    labels=["Modello 1", "Modello 2 corretto"],
    test_dates=test_dates
)

def plot_pct_change(data, col, dates):
    index = test_dates[sequence_length:]
    pct = data.pct_change()[col].values
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=index, y=pct, mode='lines', name='Variazione %', line=dict(), hovertemplate='%{x}<br>Variazione: %{y:.2%}<extra></extra>'))
    fig.add_trace(go.Scatter(x=index, y=[0]*len(index), mode='lines', name='Linea a y=0', line=dict(color='white', width=1), showlegend=True))
    fig.update_layout(
        title={'text': 'BTC/USDT - Variazione % del prezzo di chiusura in base al valore precedente', 'x': 0.5, 'xanchor': 'center',
               'font': {'size': 15, 'family': 'Arial', 'weight': 'bold'}},
        xaxis_title="Data/Ora",yaxis_title="Variazione %", yaxis=dict(tickformat=".0%"),
        legend=dict(x=0, y=1),width=600,height=600,template='plotly_dark'
    )
    fig.show()
    return fig

fig = plot_pct_change(test, 'BTC_USDT_1h_close', test.index)

"""#### Predizione per le prossime due settimane"""

n_hours_to_predict = 2 * 24 * 7  # Numero di ore da predire (1 settimana)

# Ultima sequenza completa (include target)
last_sequence = train_scaled[-sequence_length:]  # shape: (sequence_length, n_features + 1)

predictions = []

for _ in range(n_hours_to_predict):
    # Prepara l'input per il modello (rimuovi la colonna target)
    last_sequence_reduced = last_sequence[:, :-1]  # shape: (sequence_length, n_features)

    # Predizione
    next_pred = model.predict(last_sequence_reduced[np.newaxis, :, :], verbose=0)
    predictions.append(next_pred[0, 0])

    # Costruisci il nuovo timestep
    new_timestep = last_sequence[-1].copy()  # Prendi l'ultimo timestep intero
    new_timestep[target_index] = next_pred[0, 0]  # Inserisci la nuova previsione nella colonna target

    # Aggiorna la sequenza (sliding window)
    last_sequence = np.concatenate([last_sequence[1:], new_timestep[np.newaxis, :]], axis=0)

# Ricostruzione per inverse_transform
predictions_reshaped = np.zeros((len(predictions), train_scaled.shape[1]))
predictions_reshaped[:, target_index] = predictions  # Solo la colonna della target

# Inverso dello scaling
predictions = scaler.inverse_transform(predictions_reshaped)[:, target_index]

n_predictions = len(predictions)
start_date = datetime(2025, 4, 15)
date_range = [start_date + timedelta(hours=i) for i in range(n_predictions)] # Lista di timestamp per ogni ora

df_pred = pd.DataFrame({'datetime': date_range,'prediction': predictions})
df_pred.head()

fig_prev = go.Figure()
fig_prev.add_trace(go.Scatter(x=date_range,y=predictions,mode='lines',name='Forecast',line=dict(color='lightsalmon')))
fig_prev.update_layout(title={'text': "Forecast - Prossime 2 Settimane",'x': 0.5,'xanchor': 'center',
                              'font': {'size': 20,'family': 'Arial','weight': 'bold'}},
    xaxis_title="Date",yaxis_title="Price",legend=dict(x=0, y=1),template='plotly_dark',width=1200,height=700)
fig_prev.show()

"""### 2.4. Modello LSTM multi-step

Il modello autoregressivo attualmente utilizzato (par. 2.3) effettua previsioni passo dopo passo, utilizzando ogni output come input per la previsione successiva.
Questo approccio, tuttavia, comporta un effetto cumulativo degli errori: con il passare del tempo, il modello tende a “perdere il contatto” con la realtà, come si osserva chiaramente nel grafico relativo alle previsioni su due settimane.

In conclusione, il modello evidenzia una leggera tendenza al rialzo per il prezzo di chiusura del BTC nelle prossime due settimane.

#### Preparazione (train e test)
"""

def create_sequences_multi_output(data, sequence_length, n_future, target_index):
    X, y = [], []
    for i in range(len(data) - sequence_length - n_future):
        X.append(data[i:i + sequence_length, :-1])  # features
        y.append(data[i + sequence_length:i + sequence_length + n_future, target_index])  # multi-target
    return np.array(X), np.array(y)

# Parametri
sequence_length_multi = 24 * 7  # una settimana (input)
n_future = 24 * 14        # due settimane (output)
target_column = 'BTC_USDT_1h_close'
features_multi = [col for col in df_ridotto.columns if col != target_column]
target_multi_index = len(features_multi)  # target è l'ultima colonna

# Normalizzazione
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train[features_multi + [target_column]])
test_scaled = scaler.transform(test[features_multi + [target_column]])

# Creazione sequenze
X_train, y_train = create_sequences_multi_output(train_scaled, sequence_length, n_future, target_index)
X_test, y_test = create_sequences_multi_output(test_scaled, sequence_length, n_future, target_index)

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

"""#### Caricamento del modello"""

model_multi = load_model("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/modello_completo_multi.keras")
with open("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/history_multi.pkl", "rb") as file_pi:
    history_multi = pickle.load(file_pi)

y_pred_scaled = model_multi.predict(X_test)

y_pred = []
y_test_actual = []

for i in range(len(y_pred_scaled)):
    # Predizioni
    y_pred_reshaped = np.zeros((n_future, train_scaled.shape[1]))
    y_pred_reshaped[:, target_index] = y_pred_scaled[i]
    y_pred.append(scaler.inverse_transform(y_pred_reshaped)[:, target_index])

    # Valori reali
    y_test_reshaped = np.zeros((n_future, train_scaled.shape[1]))
    y_test_reshaped[:, target_index] = y_test[i]
    y_test_actual.append(scaler.inverse_transform(y_test_reshaped)[:, target_index])

y_pred = np.array(y_pred)
y_test_actual = np.array(y_test_actual)

"""#### Modello"""

tensorflow.keras.backend.clear_session()

tensorflow.random.set_seed(42)
n_features_multi = X_train.shape[2]

model = Sequential()
model.add(Input(shape=(sequence_length_multi, n_features_multi)))  # Input

# LSTM + Dense
model.add(LSTM(64, activation='relu', return_sequences=False))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(n_future))  # Output di 336 ore

# Compilazione
model.compile(optimizer='adam',loss='mse')

model.summary()

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

early_stop = EarlyStopping(patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train.reshape((y_train.shape[0], y_train.shape[1])),
    epochs=20, batch_size=32,validation_split=0.1,
    callbacks=[early_stop]
)

# Salva modello e history
model.save("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/modello_completo_multi.keras")
with open("/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/history_multi.pkl", "wb") as file_pi:
    pickle.dump(history.history, file_pi)

y_pred_scaled = model.predict(X_test)

y_pred = []
y_test_actual = []

for i in range(len(y_pred_scaled)):
    # Predizioni
    y_pred_reshaped = np.zeros((n_future, train_scaled.shape[1]))
    y_pred_reshaped[:, target_index] = y_pred_scaled[i]
    y_pred.append(scaler.inverse_transform(y_pred_reshaped)[:, target_index])

    # Valori reali
    y_test_reshaped = np.zeros((n_future, train_scaled.shape[1]))
    y_test_reshaped[:, target_index] = y_test[i]
    y_test_actual.append(scaler.inverse_transform(y_test_reshaped)[:, target_index])

y_pred = np.array(y_pred)
y_test_actual = np.array(y_test_actual)

y_test_flat = y_test.flatten()
y_pred_flat = y_pred.flatten()

mse = mean_squared_error(y_test_flat, y_pred_flat)
mae = mean_absolute_error(y_test_flat, y_pred_flat)
r2 = r2_score(y_test_flat, y_pred_flat)

# Calcolo della loss (MSE) usando direttamente il modello, se serve
loss = model.evaluate(X_test, y_test, verbose=0)

print("------ Metriche Multi-step ------")
print(f"MSE:  {mse:.4f}")
print(f"MAE:  {mae:.4f}")
print(f"R²:   {r2:.4f}")
print(f"Loss da model.evaluate(): {loss:.4f}")

"""#### Grafici"""

#@title Loss - Test vs Validation
#Per modelli caricati usare i primi (history), per modelli addestrati sul momemnto (history.history)

loss = history_multi['loss']
val_loss = history_multi['val_loss']

fig_lstm = go.Figure()
fig_lstm.add_trace(go.Scatter(x=list(range(len(loss))), y=loss, mode='lines', name='Loss'))
fig_lstm.add_trace(go.Scatter(x=list(range(len(val_loss))), y=val_loss, mode='lines', name='Validation Loss'))
fig_lstm.update_layout(
    title= {'text': "Confronto Metriche di Performance: Loss - Test vs Validation",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Epochs",yaxis_title="Loss",showlegend=True,
    width=800,height=500,template='plotly_dark')

fig_lstm.show()

print("y_pred[1].shape: ", y_pred[1].shape)
print("y_test_actual.shape: ", y_test_actual[:, 0].shape)

print(f"y_pred[:, 0] shape: {y_pred[:, 0].shape} | y_pred1[-1] shape: {y_pred[-1].shape} | y_pred1.mean(axis=1) shape: {y_pred.mean(axis=1).shape}")

"""`y_pred.shape = (2999,336)`

- Le colonne `336` rappresentano le ore all'interno dell'orizzonte di previsione, che è di una settimana (168 ore). Ogni colonna è una previsione per una specifica ora futura all'interno di quella settimana.

- Le righe `2999` rappresentano le diverse finestre temporali di partenza per la previsione.


"""

df_y_pred = pd.DataFrame(y_pred)
df_y_pred

y_pred_step1 = y_pred[:, 0] # Estrai il primo valore previsto di ogni sequenza
print(y_pred_step1.shape)

y_pred_mean = y_pred.mean(axis=1) # Calcola la media lungo i 336 step
print(y_pred_mean.shape)

# Estrai i dati da y_test_actual
if y_test_actual.ndim == 2 and y_test_actual.shape[1] == 336: #controlla se i actual è di due dimensioni e ha la seconda dim di 336
    y_test_step1 = y_test_actual[:, 0]
    y_test_mean = y_test_actual.mean(axis=1)
else: #se risulta false, allora si tratta di un modelo single-step e non multi
    y_test_step1 = y_test_actual
    y_test_mean = y_test_actual

index = test_dates[sequence_length:]
actual = test['BTC_USDT_1h_close'].values[sequence_length:]

fig = make_subplots(rows=1, cols=2, subplot_titles=('Predicted Step 1 vs Actual', 'Predicted Mean vs Actual'))

fig.add_trace(go.Scatter(x=index, y=y_test_step1, mode='lines', name='Actual', line=dict(color='white')), row=1, col=1)
fig.add_trace(go.Scatter(x=index, y=y_pred_step1, mode='lines', name='Predicted (Step 1)', line=dict(color='orange')), row=1, col=1)

fig.add_trace(go.Scatter(x=index, y=y_test_step1, mode='lines', name='Actual', line=dict(color='white'), showlegend=False), row=1, col=2)
fig.add_trace(go.Scatter(x=index, y=y_pred_mean, mode='lines', name='Predicted (Mean)', line=dict(color='salmon')), row=1, col=2)

fig.update_layout(
    title= {'text': "Confronto Previsioni vs Valori Reali",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}}
    ,showlegend=True, width=1500,height=500,template='plotly_dark')

fig.update_xaxes(title_text="Date", row=1, col=1)
fig.update_xaxes(title_text="Date", row=1, col=2)
fig.update_yaxes(title_text="Value (step 1)", row=1, col=1)
fig.update_yaxes(title_text="Value mean", row=1, col=2)

fig.show()
print("\n------------------------------------------------------------------")
print("| Lo scarto medio tra la y_media e y_step1 è di:", round(np.abs(y_pred_mean - y_pred_step1).mean(), 2), "dollari |")
print("------------------------------------------------------------------")

scarto = np.abs(y_test_actual - y_pred)
media_scarto = np.mean(scarto)
print("Lo scarto medio tra la Y reale e la predetta è di:",round(media_scarto, 2), "dollari")

y_corretta = y_pred + media_scarto
print("\nLa nuova y_corretta:", y_corretta[:3], "\n\nLa y_pred:", y_pred[:3], "\n\nLa y_test:", y_test_actual[:3])

print("y_corretta.shape:", y_corretta.shape)
y_corretta_step1 = y_corretta[:, 0]

fig = go.Figure()
fig.add_trace(go.Scatter(x=index, y=y_test_step1, mode='lines', name='Actual - Step 1', line=dict(color='white')))
fig.add_trace(go.Scatter(x=index, y=y_corretta_step1, mode='lines', name='Forecast Corretta', line=dict(color='deepskyblue')))
fig.update_layout(
    title= {'text': "Previsioni vs Valori Reali - Primo Step (1h) [y_corretta]",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Epochs",yaxis_title="Loss",showlegend=True,
    width=1200,height=700,template='plotly_dark')

fig.update_xaxes(title_text="Date")
fig.update_yaxes(title_text="Price BTC")

fig.show()

"""Esempio:

Se `y_pred.shape = (3000, 336)`, allora:
- `y_pred[0]` = la previsione dei prossimi 336 step a partire dal primo input di test.
- `y_pred[2999]` (cioè `y_pred[-1]`) = previsione a 336 step partendo dall’ultima finestra disponibile.

Spesso si usa `y_pred[-1]` quando vuoi visualizzare solo l’ultima previsione, quella più “futura”.
"""

#@title Previsione per le prossime 2
#future_forecast = y_pred[-1]  # Ultima previsione: 336 step -> per effettuare la previsione delle due future settimane
future_forecast = y_corretta[-1]
#y_pred[-1] restituisce l’ultima previsione fatta dal modello, cioè l’output relativo all’ultima finestra del tuo set di test.

last_date = test_dates.iloc[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(hours=1), periods=336, freq='h')

# Fit polinomiale
y = np.array(future_forecast)
x = np.arange(len(y))
degree = 3
coeffs = np.polyfit(x, y, deg=degree)
poly_model = np.poly1d(coeffs)
y_fit = poly_model(x)

# Residui, stderr e intervallo di confidenza
residuals = y - y_fit
stderr = np.sqrt(np.sum(residuals**2) / (len(x) - degree - 1))
alpha = 0.10  # 90% CI
t_value = t.ppf(1 - alpha/2, df=len(x) - degree - 1)
conf_interval = t_value * stderr
y_upper = y_fit + conf_interval
y_lower = y_fit - conf_interval

# Media mobile
rolling_avg = pd.Series(y).rolling(window=24).mean()

fig = go.Figure()
# Confidence interval
fig.add_trace(go.Scatter(x=future_dates, y=y_upper, mode='lines', line=dict(width=0), showlegend=False))
fig.add_trace(go.Scatter(x=future_dates, y=y_lower, mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(255, 255, 0, 0.10)', name='Conf. Interval', showlegend=True ))
# Forecast
fig.add_trace(go.Scatter(x=future_dates, y=y, mode='lines', name='Forecast', line=dict(color='deepskyblue')))
# Rolling average
fig.add_trace(go.Scatter(x=future_dates, y=rolling_avg, mode='lines', name='Media Mobile (24h)',line=dict(color='salmon')))
# Polynomial trend
fig.add_trace(go.Scatter(x=future_dates, y=y_fit, mode='lines', name=f'Polynomial Trend (deg {degree})', line=dict(color='yellow')))

fig.update_layout(
    title= {'text': "Previsione per le prossime 2 settimane con Curva Polinomiale e Intervallo di Confidenza",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Date",yaxis_title="Prezzo previsto",showlegend=True, width=1300,height=700,template='plotly_dark', legend=dict(x=0, y=1))

fig.show()

def plot_r2_sub_comparison(y_actual, y_pred1, y_pred2, label1='Predetta', label2='Corretta'):
    r2_1 = r2_score(y_actual, y_pred1)
    r2_2 = r2_score(y_actual, y_pred2)

    min_val = min(y_actual.min(), y_pred1.min(), y_pred2.min())
    max_val = max(y_actual.max(), y_pred1.max(), y_pred2.max())

    fig = make_subplots(rows=1, cols=2, subplot_titles=[f'{label1} Prediction - R² = {r2_1:.3f}',f'{label2} Prediction - R² = {r2_2:.3f}'])
    # Subplot 1: predetta
    fig.add_trace(go.Scatter(x=y_actual, y=y_pred1, mode='markers',marker=dict(color='lightsalmon', opacity=0.6),name=label1), row=1, col=1)
    fig.add_trace(go.Scatter(x=[min_val, max_val], y=[min_val, max_val],mode='lines', line=dict(color='white', dash='dash'),name='Ideal', showlegend=False), row=1, col=1)

    # Subplot 2: corretta
    fig.add_trace(go.Scatter(x=y_actual, y=y_pred2, mode='markers',marker=dict(color='deepskyblue', opacity=0.6),name=label2), row=1, col=2)
    fig.add_trace(go.Scatter(x=[min_val, max_val], y=[min_val, max_val],mode='lines', line=dict(color='white', dash='dash'),name='Ideal', showlegend=False), row=1, col=2)

    # Layout generale
    fig.update_layout(title={'text': 'Confronto Scatter Plot R² tra modelli','x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
        width=1200,height=600,legend=dict(x=0, y=1),template='plotly_dark')

    # Titoli assi
    for col in [1, 2]:
        fig.update_xaxes(title_text='Actual Values', row=1, col=col)
        fig.update_yaxes(title_text='Predicted Values', row=1, col=col)

    fig.show()

plot_r2_sub_comparison(y_test_step1, y_pred_step1, y_corretta_step1)

"""### 2.5. Classification

L’obiettivo di questo modello di classificazione è analizzare e prevedere se il prezzo del Bitcoin sarà in rialzo o in ribasso.

In conclusione, dalle analisi del modello applicato per una previsione settimanale è emerso che il sistema fatica a determinare correttamente se la chiusura del prezzo di BTC sarà superiore o inferiore rispetto all’apertura. Il modello ha raggiunto un’accuratezza di 0.49, il che corrisponde a una performance simile a quella di un modello casuale.

#### Previsioni a un giorno
"""

df_class_with_dt = pd.DataFrame()
df_class_with_dt['Datetime'] = df_lstm_with_dt['Datetime']
df_class_with_dt['BTC +1g'] = df_lstm_with_dt['BTC_USDT_1h_close'].pct_change(giorno).shift(-giorno)
df_class_with_dt['BTC -1s'] = df_lstm_with_dt['BTC_USDT_1h_close'].pct_change(settimana)
df_class_with_dt.dropna(inplace=True)

df_class = df_class_with_dt.drop(columns=['Datetime'])
df_class_with_dt.head()

target_class = 'BTC +1g'
df_class[target_class] = df_class[target_class].apply(lambda x: 1 if x > 0 else 0)
features_class = [f for f in df_class.columns if f != target_class]

df_class.head()

train_class, test_class = train_test_split(df_class, test_size=0.2, shuffle=False, random_state=42)

model_lgbmc = LGBMClassifier(verbose=0)
model_lgbmc.fit(train_class[features_class], train_class[target_class])
preds = pd.Series(model_lgbmc.predict(test_class[features_class]))

accuracy = accuracy_score(test_class[target_class], preds)
balanced_accuracy = balanced_accuracy_score(test_class[target_class], preds)
average_precision = average_precision_score(test_class[target_class], preds)

print("Precisione Media:", round(average_precision,2))
print("Accuratezza:", round(accuracy,2))
print("Punteggio di Accuratezza Bilanciato:", round(balanced_accuracy,2))
print("\n-------------------------------------------------------")
print("\n",classification_report(test_clas[target], preds))
print("-------------------------------------------------------")

"""#### Previsioni a una settimana"""

target_class = 'BTC +1s'

df_class_with_dt = df_lstm_with_dt.copy()
df_class_with_dt[target_class] = df_lstm_with_dt['BTC_USDT_1h_close'].pct_change(settimana).shift(-settimana)
df_class_with_dt['BTC -1s'] = df_lstm_with_dt['BTC_USDT_1h_close'].pct_change(settimana)
df_class_with_dt.dropna(inplace=True)

df_class_with_dt = df_class_with_dt.drop(columns=['BTC_USDT_1h_close'])
df_class = df_class_with_dt.drop(columns=['Datetime'])
df_class_with_dt.head()

"""##### Modello semplice"""

target_class = 'BTC +1s'
df_class_with_dt[target_class] = df_class_with_dt[target_class].apply(lambda x: 1 if x > 0 else 0)
features_class = [col for col in df_class_with_dt.columns if col not in ['Datetime', target_class]]

df_class_with_dt.head()

train_class, test_class = train_test_split(df_class_with_dt, test_size=0.2, shuffle=False, random_state=42)

model_lgbmc = LGBMClassifier(verbose=0)
model_lgbmc.fit(train_class[features_class], train_class[target_class])
preds = pd.Series(model_lgbmc.predict(test_class[features_class]))

accuracy = accuracy_score(test_class[target_class], preds)
balanced_accuracy = balanced_accuracy_score(test_class[target_class], preds)
average_precision = average_precision_score(test_class[target_class], preds)

print("Precisione Media:", round(average_precision,2))
print("Accuratezza:", round(accuracy,2))
print("Punteggio di Accuratezza Bilanciato:", round(balanced_accuracy,2))
print("\n-------------------------------------------------------")
print("\n",classification_report(test_class[target_class], preds))
print("-------------------------------------------------------")

"""##### Modello con Purging

Obiettivo:

Addestrare e valutare un modello di classificazione per prevedere se il prezzo di BTC salirà o scenderà nel futuro (1 se sale, 0 se scende), usando dati storici in stile time-series. Il modello viene riaddestrato ogni mese simulando uno scenario realistico di finanza.

- settimana: `168`
- Target: `pct_change(168).shift(-168)`
- Frequenza split: `n_splits = len(df) // 168`
- Gap temporale: `gap = 168`
- Modello: `LightGBM`
- Valutazione: **Predizione se BTC salirà tra 1 settimana**

"""

settimana = 168

"""**Problema della “contaminazione” dei dati nelle serie temporali**

Nel caso delle serie temporali (dove i dati sono orari) i valori a tempo t sono spesso correlati con quelli a tempo t-1, t-2, …, a causa delle dinamiche di mercato. In altre parole, se il tuo modello impara da dati che sono troppo vicini nel tempo tra il set di addestramento e il set di test, può “contaminare” il test con informazioni che appartengono al periodo successivo.

Questa contaminazione avviene perché il modello potrebbe imparare pattern che sono legati al passato immediato (es. un cambiamento di prezzo che potrebbe essere prevedibile con una finestra temporale molto stretta). In altre parole, potrebbe guardare nel futuro durante l’addestramento, violando la regola fondamentale delle serie temporali: i modelli devono fare previsioni sul futuro senza conoscere il futuro stesso.

**Perché aggiungere un gap?**

L’introduzione di un gap (o “purging”) tra il set di addestramento e quello di test serve proprio a evitare che l’algoritmo faccia previsioni su dati che sono troppo vicini nel tempo e quindi potenzialmente “sintetici”. Questo gap è una tecnica usata per garantire che i dati di test siano abbastanza distanti dai dati di addestramento, evitando che il modello si adatti a pattern che potrebbero non essere generalizzabili.

In sintesi, l’introduzione del gap riduce il rischio di data leakage (perdita di dati) tra addestramento e test, migliorando la capacità del modello di generalizzare a nuovi dati.

"""

# Parametri
df_class_with_dt[target_class] = df_class_with_dt[target_class].apply(lambda x: 1 if x > 0 else 0)
features_class = [col for col in df_class_with_dt.columns if col not in ['Datetime', target_class]]

#replace degli spazi con _, per evitare gli warnings da LGMBC
df_class_with_dt.columns = df_class_with_dt.columns.str.replace(' ', '_')
target_class = target_class.replace(' ', '_')
features_class = [f.replace(' ', '_') for f in features_class]

# Parametri TimeSeriesSplit
min_train_size = int(len(df_class_with_dt) * 0.5) # almeno 50% per il primo training
n_splits = len(df_class_with_dt) // settimana # retraining settimanale: 168 ore
tscv = TimeSeriesSplit(n_splits=n_splits, gap=settimana)

"""`gap=168`: significa che, quando creiamo uno split, il set di addestramento e il set di test non saranno immediatamente consecutivi. Verranno separati da un intervallo di 168 unità di tempo. Questo gap assicura che i dati nel set di addestramento non siano troppo vicini ai dati nel set di test.

Questo significa che tra il momento in cui l’algoritmo termina di usare i dati di addestramento e inizia a testare il modello sui dati successivi, ci sarà uno “stacco” di 168 unità di tempo (in questo caso, probabilmente 168 ore, ovvero una settimana). Questo crea una separazione tra il set di addestramento e il set di test, impedendo che i dati del set di addestramento influenzino troppo il set di test, riducendo così il rischio di contaminazione.

"""

for i, (train_index, test_index) in enumerate(tscv.split(df_class_with_dt)):
    train_dates = df_class_with_dt.iloc[train_index]['Datetime']
    test_dates = df_class_with_dt.iloc[test_index]['Datetime']

    print(f"🔁 Split {i+1}")
    print(f"  🟩 Train: {train_dates.min()} → {train_dates.max()}")
    print(f"  🟥 Test:  {test_dates.min()} → {test_dates.max()}")
    print("-" * 54)

#Addestramento modello con retraining settimanale
all_preds = []
all_true = []
all_dates = []

for train_index, test_index in tscv.split(df_class_with_dt):
    if len(train_index) < min_train_size:
        continue

    train_split = df_class_with_dt.iloc[train_index]
    test_split = df_class_with_dt.iloc[test_index]

    model = LGBMClassifier(n_estimators=100,learning_rate=0.05,max_depth=5,num_leaves=31,subsample=0.8,colsample_bytree=0.8,random_state=42,verbose=-1)
    model.fit(train_split[features_class], train_split[target_class])

    preds = model.predict(test_split[features_class])
    all_preds.extend(preds)
    all_true.extend(test_split[target_class].values)
    all_dates.extend(df_class_with_dt.iloc[test_index]['Datetime'].values)

# 📊 Metriche
all_preds = pd.Series(all_preds, name="Predictions")
all_true = pd.Series(all_true, name="True Values")
all_dates = pd.to_datetime(all_dates)
print("-" * 32)
print("📈 Average Precision Score:", round(average_precision_score(all_true, all_preds),2))
print("🎯 Accuracy Score:", round(accuracy_score(all_true, all_preds),2))
print("⚖️ Balanced Accuracy Score:", round(balanced_accuracy_score(all_true, all_preds),2))
print("-" * 32)

plot_df_class = pd.DataFrame({
    'Date': dates_all,
    'True': all_true,
    'Predicted': all_preds
})
plot_df_class.dropna(inplace=True)

hours_focus = 24 * 7 * 4 #672 ore, ovvere le ultime 4 sett
n_sett = hours_focus//(settimana)
last_hours = plot_df_class.tail(hours_focus)

fig = go.Figure()
fig.add_trace(go.Scatter(x=last_hours['Date'], y=last_hours['True'], mode='lines', name='True', line=dict(color='white', width=1)))
fig.add_trace(go.Scatter(x=last_hours['Date'], y=last_hours['Predicted'], mode='lines', name='Predicted', line=dict(color='deepskyblue', width=1)))

fig.update_layout(
    title= {'text': f"Classificazione: Previsioni vs Valori Reali, delle ultime {n_sett} settimane",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Date",yaxis_title="Class (0 = Down, 1 = Up)",yaxis=dict(tickmode='array',tickvals=[0, 1],ticktext=['0', '1']), showlegend=True,
    width=1200,height=400,template='plotly_dark')

fig.show()

cm = confusion_matrix(all_true, all_preds); labels = ["Down (0)", "Up (1)"]

fig_cm = go.Figure(data=go.Heatmap(z=cm, x=labels, y=labels, colorscale='Blues', text=cm, texttemplate="%{text}",
                                   hovertemplate="Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>"))
fig_cm.update_layout(
    title= {'text': f"Confusion Matrix",'x': 0.5,'xanchor': 'center','font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title="Predicted Label", yaxis_title="True Label", width=600, height=600, template='plotly_dark')

fig_cm.show()

"""## 3. Fear Index - Sentiment

Variabili Sentiment & psychology: Fear & Greed Index, Google Trends, BTC dominance

### 3.1. Preparazione per l'analisi

#### Preparazione dataset
"""

columns_to_keep_df_sent = []
for col in data_merged.columns:
    col_lower = col.lower()
    if any(key in col_lower for key in ['open', 'high', 'low']) and 'close' not in col_lower:
        continue
    columns_to_keep_df_sent.append(col)

df_sent_with_dt = data_merged[columns_to_keep_df_sent]
print("Colonne mantenute in df_sent_with_dt:")
print(df_sent_with_dt.columns.tolist())

df_sent_with_dt['Datetime'] = pd.to_datetime(df_sent_with_dt['Datetime'])

df_sent = df_sent_with_dt.drop(columns=['Datetime'])
df_sent.head()

df_sent.info()

"""#### Correlazione Features con Target"""

abbreviated_columns = [
    "BNB_close", "BNB_vol","BTC_close", "BTC_vol","DOGE_close", "DOGE_vol","ETH_close", "ETH_vol","SOL_close", "SOL_vol","XRP_close", "XRP_vol",
    "cattle_close", "cattle_vol","corn_close", "corn_vol","crude_close", "crude_vol","gold_close", "gold_vol","silver_close", "silver_vol","soybeans_close", "soybeans_vol","wheat_close", "wheat_vol",
    "CAC_close", "CAC_vol","DAX_close", "DAX_vol","Dow_close", "Dow_vol","EURO_close", "EURO_vol","FTSE_close", "FTSE_vol","IBOVESPA_close", "IBOVESPA_vol","IPC_close", "IPC_vol","NASDAQ_close", "NASDAQ_vol","Russell_close", "Russell_vol","S&P_close_1", "S&P_vol_1","S&P_close_2", "S&P_vol_2","VIX_close", "VIX_vol",
    "funding_rate",
    "trends_buy_crypto","trends_bitcoin"
]

# Calcolo della matrice di correlazione
corr_matrix = df_sent.corr()
target_corr = corr_matrix['fear_gread_index'].drop('fear_gread_index')
target_corr_sorted = target_corr.reindex(target_corr.abs().sort_values(ascending=False).index)

corr_df = pd.DataFrame({
    'Variabile': target_corr_sorted.index,
    'Correlazione': target_corr_sorted.values,
})


fig = px.bar(corr_df,x='Correlazione',y='Variabile',
    orientation='h',
    color='Correlazione', color_continuous_scale='RdBu')

fig.update_layout(title={'text': "Correlazione delle feature con Fear & Greed Index",'x': 0.5,
                         'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},},
                  yaxis=dict(autorange="reversed"),  height=1000, width=1200, )
fig.write_image("correlazione_fear_greed_index.png")
fig.show()

significant_features = target_corr[abs(target_corr) > 0.1].index.tolist()
print("Variabili significative:", significant_features)

"""Per iniziare l’analisi predittiva su
fear and greed index osserviamo
la matrice di correlazione tra la
variabile di interesse e le altre
features.
"""

correlazioni = df_sent.corr()
correlazioni_target = correlazioni["fear_gread_index"].sort_values(ascending=False)
top_features = correlazioni_target.abs().sort_values(ascending=False).head(30).index.tolist()
top_features

"""#### Correlazione tra le Features"""

features = df_sent.drop(columns=['fear_gread_index'])
correlation_matrix = features.corr()
original_columns = list(correlation_matrix.columns)

fig_cm = go.Figure(data=go.Heatmap(z=correlation_matrix.values,x=abbreviated_columns,y=abbreviated_columns,
        colorscale='RdBu',reversescale=False,colorbar=dict(title="Correlation")))
fig_cm.update_layout(
    title={'text': "Correlation Matrix",'x': 0.5,'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},},
    margin=dict(t=200, b=0, l=0, r=0),height=800,width=1200,)
fig_cm.show()

columns_to_drop = [col for col in features.columns if 'vol' in col.lower()]
features_no_vol = features.drop(columns=columns_to_drop)
features_no_vol.columns

abbreviated_columns_no_vol = [
    "BNB_close", "BTC_close","DOGE_close","ETH_close","SOL_close","XRP_close",
    "cattle_close","corn_close","crude_close","gold_close","silver_close","soybeans_close","wheat_close",
    "CAC_close","DAX_close","Dow_close","EURO_close","FTSE_close","IBOVESPA_close","IPC_close","NASDAQ_close","Russell_close","S&P_close_1","S&P_close_2","VIX_close",
    "funding_rate",
    "trends_buy_crypto","trends_bitcoin"
]

#df_sent = df_sent.drop(columns=columns_to_drop)
features = df_sent.drop(columns=['fear_gread_index'])

correlation_matrix = features_no_vol.corr()
original_columns = list(correlation_matrix.columns)
fig_cm = ff.create_annotated_heatmap(z=correlation_matrix.values, x=abbreviated_columns_no_vol, y=abbreviated_columns_no_vol,
    annotation_text=np.around(correlation_matrix.values, decimals=2),showscale=True, colorscale='RdBu')
fig_cm.update_layout(
    title={'text': "Correlation Matrix",'x': 0.5,'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'},},
    margin=dict(t=200, b=0, l=0, r=0),height=800,width=1500,)
fig_cm.show()

#ANALISI VIF
X = df_sent.drop(columns = ['fear_gread_index'])
X_const = sm.add_constant(X)

# Calcolo VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_const.values, i+1) for i in range(len(X.columns))]  # i+1 per saltare la costante
print(vif_data)

"""#### PCA"""

features = features_no_vol

scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

pca = PCA() #PCA senza ridurre il numero di componenti
X_pca = pca.fit(X_scaled)
explained_variance = X_pca.explained_variance_ratio_ #calcolo la varianza spiegata da ciascuna componente

# Numero di componenti che spiegano la % della varianza
percentuale_var = 0.90
cumulative_variance = X_pca.explained_variance_ratio_.cumsum()
n_components = (cumulative_variance >= percentuale_var).argmax() + 1
print(f"Numero di componenti che spiegano il {percentuale_var} della varianza: {n_components}")

fig_pca = go.Figure()

# Aggiungiamo la traccia per la "Varianza Spiegata" (sinistra)
fig_pca.add_trace(go.Scatter(x=list(range(1, len(explained_variance) + 1)), y=explained_variance,
                             mode='lines+markers', marker=dict(color='#636EFA'), name='Varianza Spiegata',
                             hovertemplate='%{x}<br>Varianza Spiegata: %{y:.4f}<extra></extra>'))

# Aggiungiamo la traccia per la "Varianza Cumulativa" (destra)
fig_pca.add_trace(go.Scatter(x=list(range(1, len(explained_variance) + 1)), y=explained_variance.cumsum(),
                             mode='lines+markers', marker=dict(color='#EF553B'), name='Varianza Cumulativa',
                             hovertemplate='%{x}<br>Varianza Cumulativa: %{y:.4f}<extra></extra>',
                             yaxis="y2"))

# Linea orizzontale a 95% sulla y2
y_value_percent = percentuale_var * explained_variance.sum()  # 95% del totale della varianza cumulativa
#fig_pca.add_trace(go.Scatter(x=[1, len(explained_variance)], y=[y_value_percent, y_value_percent],mode='lines', line=dict(color='gray', dash='dash', width=1), name=f'Linea {round(y_value_percent,4)*100}% Varianza Cumulativa'))
fig_pca.add_shape(type='line',y0=percentuale_var,y1=percentuale_var,x0=0,x1=1,xref='paper', yref='y2',line=dict(color='gray', dash='dash', width=1), name=f'Linea {round(y_value_percent,4)*100}% Varianza Cumulativa')

#punto di intersezione tra la curva di varianza cumulativa e la linea orizzontale
cumulative_variance = explained_variance.cumsum()
x_intersection = np.argmax(cumulative_variance >= y_value_percent) + 1  # Aggiungiamo 1 perché gli indici partono da 0
y_intersection = cumulative_variance[x_intersection - 1]  # Otteniamo il valore della varianza cumulativa al punto di intersezione

#Linea verticale perpendicolare (passante per il punto di intersezione)
fig_pca.add_trace(go.Scatter(x=[x_intersection, x_intersection], y=[0, y_intersection],
                             mode='lines', line=dict(color='gray', dash='dash', width=1), name='Linea Perpendicolare', showlegend=False))

fig_pca.update_layout(
    title={'text': "Analisi PCA - Varianza per Componente", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    showlegend=True, width=1200, height=600,
    yaxis=dict(title="Varianza Spiegata", titlefont=dict(color="#636EFA")),
    yaxis2=dict(title="Varianza Cumulativa", titlefont=dict(color="orangered"), overlaying="y", side="right"),
    xaxis=dict(title="Numero di Componente",dtick=1, tickvals=list(range(1, len(explained_variance) + 1)) ),
    legend=dict(x=1.05, xanchor='left', yanchor='middle'),
    template='plotly_dark'
)

fig_pca.show()

components = X_pca.components_[:n_components] # Prendi i carichi delle prime 'n_components' componenti
var_names = features.columns
pesi_var_pca = pd.DataFrame(components, columns=var_names) #DataFrame con i carichi delle variabili
pesi_var_pca.round(4)
print("Carichi delle variabili per le componenti selezionate:")
pesi_var_pca

#Le variabili più significative per ogni componente
for i in range(n_components):
    top_features = pesi_var_pca.iloc[i].sort_values(ascending=False).head(30)
    print(f"\nLe variabili più significative per la componente {i+1}:")
    print(top_features)

"""1. Global Market Movement (S&P, Dow, NASDAQ, BTC)
 - **Motivazione**: La Componente 1 è fortemente influenzata da indici azionari globali come il S&P 500, il Dow Jones, il NASDAQ e Bitcoin. Queste variabili suggeriscono una forte correlazione con i movimenti di mercato globali, in particolare quelli azionari e di criptovalute. Le principali variabili in questa componente sono tutte relative a indici di mercato finanziario, indicando che questa componente cattura le tendenze generali del mercato.

 - **Implicazioni**: Questa componente riflette i movimenti di mercato globali e può essere utile per analizzare l’andamento complessivo degli asset tradizionali e di criptovalute.

2. Market Sentiment and Funding Rates
 - **Motivazione**: La Componente 2 è dominata da variabili come il funding rate, l’IPC (Indice dei Prezzi al Consumo) e il VIX (Indice di Volatilità), che misurano il sentiment del mercato e la percezione del rischio. Questi fattori sono spesso utilizzati per analizzare la stabilità del mercato e l’andamento dell’inflazione e dei tassi di interesse.

 - **Implicazioni**: Questa componente suggerisce che le variabili legate al sentiment e alla percezione del rischio sono cruciali per determinare il comportamento degli asset finanziari, con un’enfasi sul rischio sistemico (rappresentato dal VIX) e sul rischio di liquidità (rappresentato dal funding rate).

3. Crypto Search Trends and Sentiment
 - **Motivazione**: La Componente 3 è influenzata principalmente dalle tendenze di ricerca su Google relative a criptovalute come Bitcoin e Dogecoin, nonché dal sentiment del mercato misurato tramite google_trends_buy_crypto. Questo suggerisce che questa componente cattura la reattività del mercato e delle ricerche online rispetto alle criptovalute, un fenomeno che è diventato sempre più importante nelle decisioni di investimento.

 - **Implicazioni**: Questa componente riflette l’adozione e l’interesse del pubblico nelle criptovalute, indicando come il sentiment e le tendenze di ricerca online possano influenzare l’andamento del mercato crypto.

4. Commodity and Equity Market Dynamics
 - **Motivazione**: La Componente 4 è dominata da commodity come il corn, il wheat e il CAC 40 (indice azionario francese), insieme a XRP (una criptovaluta) e vari altri indici azionari. Questi fattori suggeriscono una correlazione tra l’andamento dei mercati azionari e delle commodities, con un focus sulle dinamiche dei mercati internazionali.

 - **Implicazioni**:Questa componente potrebbe essere utilizzata per analizzare come i mercati delle commodities e delle azioni interagiscono e influenzano l’economia globale, con una particolare attenzione agli sviluppi in Europa e in mercati emergenti.

5. Commodities Price Movements
 - **Motivazione**: La Componente 5 è fortemente influenzata da commodities come il crude oil (petrolio), il cattle (bovini), e il wheat (grano), ma anche da indici azionari come il CAC 40. Questi fattori mostrano una correlazione forte tra i prezzi delle commodities e i mercati finanziari, indicando che il movimento dei prezzi delle materie prime potrebbe avere un impatto significativo sui mercati azionari.

 - **Implicazioni**: Questa componente può essere utilizzata per analizzare i movimenti dei prezzi delle commodities e la loro relazione con i mercati azionari e le economie emergenti. È particolarmente utile per chi investe in materie prime o per analizzare i trend economici globali.

6. Emerging Market and Macro Trends
 - **Motivazione**: La Componente 6 è dominata da variabili legate agli emerging markets (mercati emergenti) come l’IBOVESPA (indice brasiliano) e l’IPC (Indice dei Prezzi al Consumo). Questi fattori suggeriscono che questa componente riflette le dinamiche macroeconomiche e i mercati emergenti, con particolare attenzione ai fattori inflazionistici e ai tassi di interesse.

 - **Implicazioni**: Questa componente è utile per analizzare l’andamento dei mercati emergenti e l’impatto di variabili macroeconomiche su questi mercati. È importante per chi è interessato agli investimenti nei mercati emergenti o agli sviluppi macroeconomici globali.
"""

X_pca = pca.transform(X_scaled)  # Trasformazione delle componenti principali con PCA
pca_df = pd.DataFrame(X_pca[:, :n_components], columns=[f'PC{i+1}' for i in range(n_components)])

pca_component_names = [
    "Global Market Movement",                           # Componente 1
    "Market Sentiment and Funding Rates",               # Componente 2
    "Crypto Search Trends and Sentiment",               # Componente 3
    "Commodity and Equity Market Dynamics",             # Componente 4
    "Commodities Price Movements",                      # Componente 5
    "Emerging Market and Macro Trends"                  # Componente 6
]

pca_df.columns = pca_component_names[:n_components] # Rinomina le colonne
pca_df['fear_gread_index'] = df_sent['fear_gread_index'].values
pca_df.head()

pca_df.info()

#ANALISI VIF
X = pca_df.drop(columns = ['fear_gread_index'])
X_const = sm.add_constant(X)

# Calcolo VIF
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_const.values, i+1) for i in range(len(X.columns))]  # i+1 per saltare la costante
print(vif_data)

#Verifica se ha mantenuto l'ordine delle righe
print(df_sent['fear_gread_index'].head(-5))
print(pca_df['fear_gread_index'].head(-5))

df_ridotto = pca_df

"""- Componente 1: Rappresenta i movimenti generali dei mercati finanziari globali.
- Componente 2: Rappresenta il sentiment di mercato, inclusi rischi sistemici e inflazione.
- Componente 3: Rappresenta l'interesse e il sentiment delle criptovalute, attraverso le ricerche online.
- Componente 4: Analizza la dinamica tra commodities e mercati azionari.
- Componente 5: Si concentra sulle fluttuazioni dei prezzi delle commodities e il loro impatto sui mercati finanziari.
- Componente 6: Rappresenta le tendenze macroeconomiche e i mercati emergenti.

#### Train and test
"""

y = pca_df['fear_gread_index']
X = pca_df.drop(columns=['fear_gread_index'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=343)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print("La shape di X è:", X.shape)
print("La shape di y è:", y.shape)

"""### 3.2. Analisi

#### Confronto Algorimti di regressione

Come introduzione
sono stati testati
modelli di
regressione e sono
indicati media e
MSE nel grafico.
L’algoritmo scelto è
stato il «Random
Forest» dato che
presentava un
R_squared
maggiore ed un
MSE minore.
"""

models = [
    ('Linear Regression', LinearRegression()),
    ('Ridge Regression', Ridge()),
    ('Lasso Regression', Lasso()),
    ('Elastic Net Regression', ElasticNet()),
    ('Decision Tree Regression', DecisionTreeRegressor()),
    ('Random Forest Regression', RandomForestRegressor()),
    ('Gradient Boosting Regression', GradientBoostingRegressor()),
    ('AdaBoost Regression', AdaBoostRegressor()),
    ('Support Vector Regression', SVR()),
    ('K-Nearest Neighbors Regression', KNeighborsRegressor()),
    ('MLP Regression', MLPRegressor()),
    ('Huber Regression', HuberRegressor()),
    ('Quantile Regression', QuantileRegressor()),
    ('RANSAC Regression', RANSACRegressor()),
    ('TheilSen Regression', TheilSenRegressor()),
    ('Bayesian Ridge Regression', BayesianRidge())
]

results_table = []
mse_all = []
model_names_all = []

n_folds = 5 # Numero di fold

for name, model in models:
    print(f"Testing model: {name}")
    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    pipeline = make_pipeline(StandardScaler(), model)
    r2_scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring="r2", n_jobs=-1)
    r2_mean = r2_scores.mean()
    mse_scores = -cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring="neg_mean_squared_error", n_jobs=-1)
    mse_mean = mse_scores.mean()
    results_table.append({
        'Modello': name,
        'R2 medio': round(r2_mean, 4),
        'MSE medio': round(mse_mean, 4)
    })
    mse_all.extend(mse_scores)
    model_names_all.extend([name] * len(mse_scores))

results_df = pd.DataFrame(results_table).sort_values(by='R2 medio', ascending=False)
print("Risultati cross-validation:")
print(results_df)

results_df_sorted = results_df.sort_values(by='MSE medio', ascending=True)

fig = go.Figure()
fig.add_trace(go.Scatter(x=results_df_sorted['Modello'],y=results_df_sorted['MSE medio'],mode='lines+markers+text',
    name='MSE medio',text=[f"{val:.2f}" for val in results_df_sorted['MSE medio']],
    textposition='top center',line=dict( width=3),yaxis='y1'))
fig.add_trace(go.Scatter(x=results_df_sorted['Modello'],y=results_df_sorted['R2 medio'],mode='lines+markers+text',
    name='R² medio',text=[f"{val:.2f}" for val in results_df_sorted['R2 medio']],
    textposition='bottom center',line=dict( width=3, dash='dot'),yaxis='y2'))

fig.update_layout(
    title={'text': "Confronto tra modelli: MSE medio e R² medio", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis=dict(title="Modello"),yaxis=dict(title="MSE medio"),
    yaxis2=dict(title="R² medio",overlaying="y",side="right"),
    legend=dict(title="Metriche", orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    height=600,width=1200, template='plotly_dark')

fig.show()

best_model = results_df_sorted.iloc[0]
print("Il modello migliore è:\n", best_model)

"""#### Analisi del miglior algorimto: Random Forest

Per ulteriori approfondimenti, nel modello precedente di «Random Forest»
si è introdotta la Grid Search con cross-validation per cercare la
combinazione ottimale di iperparametri.
"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

rf = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [10, 50, 100, 200],  # Numero di alberi
    'max_depth': [None, 10, 20, 50],  # Profondità massima degli alberi
    'min_samples_split': [2, 5, 10],  # Minimo numero di campioni per dividere un nodo
    'min_samples_leaf': [ 2, 5, 10],  # Minimo numero di campioni in una foglia
    'bootstrap': [True],  # Se usare il campionamento con sostituzione
}

grid_search = GridSearchCV(rf,param_grid,cv=3,scoring='r2',n_jobs=-1,verbose=1)
grid_search.fit(X_train_scaled, y_train)
best_rf = grid_search.best_estimator_

import pickle
with open('/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/best_rf_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)

with open('/content/drive/MyDrive/Data Science/Data Science and Generative AI/Progetto finale/best_rf_model.pkl', 'rb') as file:
    best_rf_model = pickle.load(file)

y_pred = best_rf.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R^2: {r2:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print("-"*134)
#print("Migliori parametri trovati:", grid_search.best_params_)
print("Migliori parametri trovati: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}")
print("-"*134)

fig = go.Figure()
fig.add_trace(go.Scatter(x=y_test,y=y_pred,mode='markers',name='Predizioni',marker=dict( opacity=0.6)))
min_val = min(min(y_test), min(y_pred))
max_val = max(max(y_test), max(y_pred))
fig.add_trace(go.Scatter(x=[min_val, max_val],y=[min_val, max_val],mode='lines',name='Perfetto (y = x)',line=dict(dash='dash')))

fig.update_layout(
    title={'text': "Confronto tra valori reali e predetti", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    xaxis_title='Valori reali (y_test)', yaxis_title='Valori predetti (y_pred)',
    width=800,height=600,template='plotly_dark')

fig.show()

labels = ["Global Market Movement","Market Sentiment and Funding Rates","Crypto Search Trends and Sentiment","Commodity and Equity Market Dynamics","Commodities Price Movements","Emerging Market and Macro Trends"]

fig = make_subplots(rows=2, cols=3, subplot_titles=labels)
for i in range(6):
    row = i // 3 + 1
    col = i % 3 + 1
    fig.add_trace(go.Scatter(x=X.iloc[:, i],y=y,mode='markers',marker=dict(color='lightblue', size=4),name=labels[i],showlegend=False),row=row,col=col)

fig.update_layout(
    title={'text': "Relazione tra variabili indipendenti e variabile dipendente", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    template='plotly_dark',height=700,width=1200)

fig.update_annotations(font_size=10)
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False)

fig.show()

fig = make_subplots(rows=2, cols=3, subplot_titles=labels)
for i in range(6):
    row = i // 3 + 1
    col = i % 3 + 1
    fig.add_trace(go.Scatter(x=X_test[:, i],y=y_pred,mode='markers',marker=dict(color='deepskyblue', size=4),name=labels[i],showlegend=False),row=row,col=col)

fig.update_layout(
    title={'text': "Relazione tra variabili indipendenti e variabile dipendente PREDETTA", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    template='plotly_dark',height=700,width=1200)

fig.update_annotations(font_size=10)
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False)

fig.show()

"""#### Comprendere la relazione tra target e features"""

#@title Confronto: Regressione Polinomiale vs LOWESS
grado = 3
labels = ["Global Market Movement","Market Sentiment and Funding Rates","Crypto Search Trends and Sentiment","Commodity and Equity Market Dynamics","Commodities Price Movements","Emerging Market and Macro Trends"]
fig = make_subplots(rows=2, cols=3, subplot_titles=labels)

for i in range(6):
    x = X.iloc[:, i].values.reshape(-1, 1)
    y_vals = y.values

    # -------------------------------
    # 🔸 Regressione Polinomiale
    grado = 3
    poly = PolynomialFeatures(degree=grado)
    x_poly = poly.fit_transform(x)
    model = LinearRegression().fit(x_poly, y_vals)

    x_range = np.linspace(x.min(), x.max(), 300).reshape(-1, 1)
    x_range_poly = poly.transform(x_range)
    y_pred_poly = model.predict(x_range_poly)

    # -------------------------------
    # 🟢 LOWESS (regressione locale)
    lowess = sm.nonparametric.lowess
    smoothed = lowess(y_vals, x.flatten(), frac=0.3)  # frac controlla lo smoothing

    # -------------------------------
    # Tracce
    row = i // 3 + 1
    col = i % 3 + 1

    # Dati ,marker=dict( opacity=0.6)
    fig.add_trace(go.Scatter(x=x.flatten(), y=y_vals,mode='markers', marker=dict(size=3, color='deepskyblue', opacity=0.3),name='Dati',showlegend=(i == 0)),row=row, col=col)

    # Polinomiale
    fig.add_trace(go.Scatter(x=x_range.flatten(), y=y_pred_poly,mode='lines',line=dict(color='orange', width=2),name=f'Polinomiale di grado: {grado}',showlegend=(i == 0)),row=row, col=col)
    # LOWESS
    fig.add_trace(go.Scatter(x=smoothed[:, 0], y=smoothed[:, 1],mode='lines',line=dict(color='white', width=2),name='LOWESS',showlegend=(i == 0)),row=row, col=col)

    fig.update_yaxes(title_text="Fear & Greed Index", row=row, col=col)

fig.update_layout(
    title={'text': f"Confronto: Regressione Polinomiale (grado {grado})  vs LOWESS", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    template='plotly_dark',height=800,width=1500, )

fig.update_annotations(font_size=10)
fig.update_xaxes(showgrid=True)
fig.update_yaxes(showgrid=True)

fig.show()

"""Per comprenere la forma sono stata utilizzate la regressione polinomiale e la tecnica LOWESS

**Regressione Polinomiale**:
è una forma di regressione lineare che utilizza **potenze della variabile indipendente** per modellare relazioni curve.
Grado 2: Modella una curva a forma di U o ∩ (parabola). Grado 3+: Modella curve più complesse, ma può causare overfitting.

- **Pro:**
 - Interpretabile e parametrico.
 - Funziona bene quando la forma della curva è relativamente semplice.

- **Contro:**
 - Assume una **forma globale fissa** (es. parabola).
 - Sensibile a **outlier** e **overfitting** se il grado è troppo alto.

**LOWESS** (Locally Weighted Scatterplot Smoothing): è una tecnica di **regressione non parametrica** che costruisce una curva smooth **senza assumere una forma matematica predefinita**.

Per ogni punto \( x_i \), LOWESS:
1. Considera un numero di punti vicini (definito da `frac`).
2. Fitta una regressione lineare locale con **pesi** → i punti più vicini contano di più.
3. Ripete per ogni punto, ottenendo una curva morbida che segue i dati.

- **Pro:**
 - Si adatta **localmente** alla forma dei dati.
 - Eccellente per esplorazione visiva di tendenze complesse.
 - Funziona bene anche con **molto rumore**.

- **Contro:**
 - Più lento e computazionalmente costoso.
 - Non fornisce una formula esplicita del modello.
 - Difficile da usare per **predizioni future**.

**Quando usare quale?**

| Scenario                            | Reg. Polinomiale | LOWESS        |
|-------------------------------------|------------------|---------------|
| Relazione semplice (es. curva a U)  | ✅                | ✅             |
| Rumore elevato nei dati             | ⚠️ (sensibile)   | ✅             |
| Interpretabilità del modello        | ✅                | ⚠️ (no formula) |
| Predizioni fuori dal dataset        | ✅                | ⚠️ (non adatto) |
| Esplorazione visiva                 | ✅                | ✅             |
| Tendenza locale non uniforme        | ⚠️                | ✅             |



**Conclusione**

- Si usa la **regressione polinomiale** quando si sospetta una relazione curva ma coerente e se si vuole un modello matematico.
- Mentre, si usa **LOWESS** quando l'obiettivo è  **seguire l’andamento reale dei dati** senza imporre una forma specifica, soprattutto in fase di **esplorazione**.

##### Altri grafici prova
"""

fig = make_subplots(rows=2, cols=3, subplot_titles=labels)
for i in range(6):
    x = X.iloc[:, i].values.reshape(-1, 1)
    y_vals = y.values
    grado = 3
    poly = PolynomialFeatures(degree=grado)
    x_poly = poly.fit_transform(x)
    model = LinearRegression().fit(x_poly, y_vals)

    x_range = np.linspace(x.min(), x.max(), 300).reshape(-1, 1)
    x_range_poly = poly.transform(x_range)
    y_pred = model.predict(x_range_poly)

    row = i // 3 + 1
    col = i % 3 + 1

    fig.add_trace(go.Scatter(x=x.flatten(), y=y_vals,mode='markers',marker=dict(size=3, color='lightblue'),name='Dati', showlegend=False),row=row, col=col)

    # Curva polinomiale
    fig.add_trace(go.Scatter(x=x_range.flatten(), y=y_pred,mode='lines',line=dict(color='orange', width=2),name='Polinomiale',showlegend=False),row=row, col=col)



fig.update_layout(
    title={'text': f"Relazioni Polinomiali (grado {grado}) tra Variabili Indipendenti e y", 'x': 0.5, 'xanchor': 'center', 'font': {'size': 20, 'family': 'Arial', 'weight': 'bold'}},
    template='plotly_dark',height=700,width=1200)

fig.update_annotations(font_size=10)
fig.update_xaxes(showgrid=True)
fig.update_yaxes(showgrid=True)

fig.show()

lowess = sm.nonparametric.lowess
smoothed = lowess(y_vals, x.flatten(), frac=0.3)  # frac = quanto smoothing

fig = go.Figure()
fig.add_trace(go.Scatter(x=x.flatten(), y=y_vals, mode='markers', name='Dati', marker=dict(color='lightblue')))
fig.add_trace(go.Scatter(x=smoothed[:, 0], y=smoothed[:, 1], mode='lines', name='LOWESS', line=dict(color='lime')))
fig.update_layout(template='plotly_dark', title='LOWESS Fit', title_x=0.5)
fig.show()

"""## Conclusioni

**Analisi Descrittiva**

L’analisi ha confrontato l’andamento e i rendimenti del Bitcoin (BTC) con quelli di altri asset finanziari — tra cui criptovalute, indici di mercato e materie prime — su orizzonti temporali di 6 mesi e 1 anno. Il BTC ha evidenziato una crescita costante e rendimenti superiori rispetto agli indici tradizionali, sebbene accompagnati da una maggiore volatilità, in particolare nel breve periodo. Nel complesso, il BTC presenta una correlazione positiva con la maggior parte dei mercati analizzati, ad eccezione di alcuni asset specifici — in particolare soia, grano, mais, petrolio greggio e l’Indice dei Prezzi al Consumo (IPC) — con i quali mostra una correlazione negativa, elencati in ordine decrescente di intensità.

**Analisi Predittiva**

Per l'analisi predittica sul market trend:
- I modelli LSTM mostrano buona capacità predittiva a breve termine, ma scarsa generalizzazione a lungo termine senza l’integrazione di dati esterni.
- I modelli di classificazione non hanno superato le performance di un modello casuale, confermando l’insufficienza dei soli dati storici per la previsione del market trend.

**Una possibile Analisi Futura**. Per migliorare l'approccio predittivo, si consiglia di: integrare fattori esogeni come cambiamenti nei tassi d’interesse, politiche monetarie o eventi geopolitici. Ad esempio, eventi economici globali, politiche monetarie, e notizie legate a Bitcoin. Ciò permetterebbe la costruzione di modelli scenario-based, in grado di simulare l’impatto di specifici eventi futuri sul sentiment di mercato.

Il **Fear and Greed Index** è un indicatore che misura il sentiment di mercato, oscillando tra due estremi: paura (bassi livelli) e avidità (alti livelli). Esso è tendenzialmente influenzato da diverse variabili di mercato, come ad esempio:

- **Global Market Movement**, si osserva un'onda tendenzialmente crescente, suggerendo che i movimenti globali dei mercati influenzano in modo dinamico il sentiment, con un focus crescente sull’andamento complessivo dei mercati finanziari.

- **Market Sentiment and Funding Rates** e **Crypto Search Trends and Sentiment** tendono a aumentare il Fear and Greed Index, poiché un sentiment positivo e la crescita delle ricerche sugli asset digitali indicano un clima di ottimismo.

- **Commodity and Equity Market Dynamics** mostra una curva a U rovesciata, dove in periodi di alta volatilità o turbolenze, il sentiment di paura aumenta, mentre in fasi di stabilità cresce l'avidità.

- **Commodities Price Movements** tende a scendere parallelamente al Fear and Greed Index, suggerendo che fluttuazioni significative positive nei prezzi delle materie prime, spesso legate a periodi di incertezza, spingano gli investitori verso la paura.

- Infine, con **Emerging Market and Macro Trends** generano una curva a U tendenzialmente piatta, con i movimenti nei mercati emergenti e nei dati macroeconomici che mostrano una relazione meno definita, ma comunque correlata a cicli economici e trend globali.
"""